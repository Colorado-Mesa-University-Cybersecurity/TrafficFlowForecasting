{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import fastai\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from datetime import (\n",
    "    datetime\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler\n",
    ")\n",
    "\n",
    "from typing import (\n",
    "    Callable,\n",
    "    Dict,\n",
    "    List,\n",
    "    Tuple\n",
    ")\n",
    "\n",
    "from Quick.cleaning.loading import (\n",
    "    examine_dataset,\n",
    "    remove_infs_and_nans\n",
    ")\n",
    "\n",
    "from Quick.cleaning.utils import (\n",
    "    get_file_path\n",
    ")\n",
    "\n",
    "from Quick.runners.deep import (\n",
    "    run_deep_nn_experiment\n",
    ")\n",
    "\n",
    "from Quick.runners.residual import (\n",
    "    run_residual_deep_nn_experiment\n",
    ")\n",
    "\n",
    "from Quick.runners.sk import (\n",
    "    run_sk_experiment\n",
    ")\n",
    "\n",
    "from Quick.runners.torch import (\n",
    "    run_torch_nn_experiment\n",
    ")\n",
    "\n",
    "from rff.layers import (\n",
    "    GaussianEncoding,\n",
    ")\n",
    "\n",
    "\n",
    "from Quick.constants import *\n",
    "\n",
    "pretty = pprint.PrettyPrinter(indent=4).pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are currently using the device: cpu\n"
     ]
    }
   ],
   "source": [
    "use_gpu: bool = False\n",
    "\n",
    "if(use_gpu):\n",
    "\n",
    "    if(torch.backends.mps.is_available()): # For Mac M1/M2 chips\n",
    "        device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    else: # For NVIDIA cuda chips\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'We are currently using the device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are currently using the CICDDoS-2019 dataset from the Canadian Institute of Cybersecurity, found [here](https://www.unb.ca/cic/datasets/ddos-2019.html). \n",
    "\n",
    "Download the csv files in zip form from the website and extract the zip in the ..data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datasets we will be working with are:\n",
      "[   '../data/01-12/DrDoS_NTP.csv',\n",
      "    '../data/01-12/DrDoS_DNS.csv',\n",
      "    '../data/01-12/DrDoS_LDAP.csv',\n",
      "    '../data/01-12/DrDoS_MSSQL.csv',\n",
      "    '../data/01-12/DrDoS_NetBIOS.csv',\n",
      "    '../data/01-12/DrDoS_SNMP.csv',\n",
      "    '../data/01-12/DrDoS_SSDP.csv',\n",
      "    '../data/01-12/DrDoS_UDP.csv',\n",
      "    '../data/01-12/UDPLag.csv',\n",
      "    '../data/01-12/Syn.csv',\n",
      "    '../data/01-12/TFTP.csv']\n",
      "and\n",
      "[   '../data/03-11/Portmap.csv',\n",
      "    '../data/03-11/NetBIOS.csv',\n",
      "    '../data/03-11/LDAP.csv',\n",
      "    '../data/03-11/MSSQL.csv',\n",
      "    '../data/03-11/UDP.csv',\n",
      "    '../data/03-11/UDPLAG.csv',\n",
      "    '../data/03-11/SYN.csv']\n"
     ]
    }
   ],
   "source": [
    "ddos_data_path_1: str = '../data/01-12/'\n",
    "ddos_data_path_2: str = '../data/03-11/'\n",
    "\n",
    "# Raw datasets, still contain infs/nans/empty and are all primarily 1 class\n",
    "ddos_data_1: list = [ # https://www.unb.ca/cic/datasets/ddos-2019.html\n",
    "    'DrDoS_NTP.csv',\n",
    "    'DrDoS_DNS.csv',\n",
    "    'DrDoS_LDAP.csv',\n",
    "    'DrDoS_MSSQL.csv',\n",
    "    'DrDoS_NetBIOS.csv',\n",
    "    'DrDoS_SNMP.csv',\n",
    "    'DrDoS_SSDP.csv',\n",
    "    'DrDoS_UDP.csv',\n",
    "    'UDPLag.csv',\n",
    "    'Syn.csv',\n",
    "    'TFTP.csv',\n",
    "]\n",
    "\n",
    "# Raw datasets, still contain infs/nans/empty and are all primarily 1 class\n",
    "ddos_data_2: list = [ # https://www.unb.ca/cic/datasets/ddos-2019.html\n",
    "    'Portmap.csv',\n",
    "    'NetBIOS.csv',\n",
    "    'LDAP.csv',\n",
    "    'MSSQL.csv',\n",
    "    'UDP.csv',\n",
    "    'UDPLAG.csv',\n",
    "    'SYN.csv'\n",
    "]\n",
    "\n",
    "\n",
    "# Now we package the datasets and load when necessary\n",
    "ddos_path_1: callable = get_file_path(ddos_data_path_1)\n",
    "ddos_path_2: callable = get_file_path(ddos_data_path_2)\n",
    "\n",
    "ddos_datasets_1: list = list(map(ddos_path_1, ddos_data_1))\n",
    "ddos_datasets_2: list = list(map(ddos_path_2, ddos_data_2))\n",
    "\n",
    "print(f'The datasets we will be working with are:')\n",
    "pretty(ddos_datasets_1)\n",
    "print('and')\n",
    "pretty(ddos_datasets_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "We define a class to handle the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    map_index_key = '__map_index__'\n",
    "    category_map = {}\n",
    "    old_maps = []\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline: List[Callable[[pd.DataFrame], pd.DataFrame]],\n",
    "        device: torch.device,\n",
    "    ) -> None:\n",
    "\n",
    "        self.pipeline = pipeline\n",
    "        self.device = device\n",
    "\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, leave_out: List[str] = ['Timestamp']):\n",
    "\n",
    "        X = X.copy()\n",
    "        for col in leave_out:\n",
    "            if col in X.columns:\n",
    "                X = X.drop(columns=[col])\n",
    "\n",
    "        for step in self.pipeline:\n",
    "            step.fit(X)\n",
    "\n",
    "        self.fitted = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(\n",
    "        self, \n",
    "        X: pd.DataFrame,\n",
    "        leave_out: List[str] = ['Timestamp']\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        if not self.fitted:\n",
    "            raise Exception('E1: You must fit the preprocessor before transforming')\n",
    "    \n",
    "        X = X.copy()\n",
    "        \n",
    "        if leave_out != []:\n",
    "            left_out = X[leave_out]\n",
    "            X = X.drop(columns=leave_out)\n",
    "\n",
    "        columns = X.columns\n",
    "\n",
    "        for col in leave_out:\n",
    "            if col in columns:\n",
    "                raise Exception('E2: Undesired column %s was found in the transformed dataset' % col)\n",
    "\n",
    "        for step in self.pipeline:\n",
    "            X = step.transform(X)\n",
    "\n",
    "        X = pd.DataFrame(X, columns=columns)\n",
    "\n",
    "        if leave_out != []:\n",
    "            for col in leave_out:\n",
    "                X[col] = left_out[col]\n",
    "            # X[leave_out] = left_out\n",
    "\n",
    "        for col in leave_out:\n",
    "            if col not in X.columns:\n",
    "                raise Exception('E3: Column %s was not found in the transformed dataset' % col)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def index_categories(\n",
    "        self, \n",
    "        X: pd.DataFrame, \n",
    "        categorical_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        '''\n",
    "            We need to index the categorical columns so that they are in the range [0, n_categories) and save the mapping\n",
    "        '''\n",
    "        X = X.copy()\n",
    "\n",
    "        if self.map_index_key in categorical_cols:\n",
    "            raise Exception('Cannot use the reserved key %s as a column name' % self.map_index_key)\n",
    "\n",
    "        if self.category_map != {}:\n",
    "            self.old_maps.append(self.category_map)\n",
    "        \n",
    "        old_mapping_index = len(self.old_maps)\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            X[col], self.category_map[col] = pd.factorize(X[col])\n",
    "            self.category_map[self.map_index_key] = old_mapping_index    \n",
    "\n",
    "        return X\n",
    "\n",
    "    def create_dataset(\n",
    "        self, \n",
    "        dataset: pd.DataFrame,\n",
    "        categorical_cols: List[str],\n",
    "        relative_cols: List[str],\n",
    "        other_cols: Dict[str, List[str]],\n",
    "        context_sequence_length: int = 1,\n",
    "        target_sequence_length: int = 1,\n",
    "        target_sequence_offset: int = 0,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        '''\n",
    "            We create a dataset for each column group where each shares the same index\n",
    "            in the first dimension\n",
    "\n",
    "            This is because the groups will have different lengths in the last dimension\n",
    "            \n",
    "\n",
    "            We also need to create a context and target sequence for each column group\n",
    "                where the context sequence is the input and the target sequence is the output\n",
    "            \n",
    "                for each sample s in the dataset:\n",
    "                    context_sequence = df[s:s+context_sequence_length]\n",
    "                    target_sequence = df[s+target_sequence_offset:s+target_sequence_offset+target_sequence_length]\n",
    "\n",
    "\n",
    "        '''\n",
    "        \n",
    "        if not self.fitted:\n",
    "            self.fit(\n",
    "                dataset, \n",
    "                leave_out=relative_cols + categorical_cols\n",
    "            )\n",
    "\n",
    "        dataset = self.transform(\n",
    "            dataset,\n",
    "            leave_out=relative_cols + categorical_cols\n",
    "        )\n",
    "\n",
    "        bounds = max(\n",
    "            context_sequence_length, \n",
    "            target_sequence_length + target_sequence_offset\n",
    "        )\n",
    "\n",
    "        dataset_groups = {}\n",
    "\n",
    "        for i, (name, group) in enumerate(other_cols.items()):\n",
    "            dataset_groups[name] = dataset[group]\n",
    "\n",
    "        categorical_data = dataset[categorical_cols]\n",
    "        categorical_data = self.index_categories(categorical_data, categorical_cols)\n",
    "\n",
    "        relative_data = dataset[relative_cols]\n",
    "\n",
    "        output = {}\n",
    "\n",
    "        src = '_source'\n",
    "        tgt = '_target'\n",
    "\n",
    "        output['categorical_data' + src] = []\n",
    "        output['categorical_data' + tgt] = []\n",
    "        output['relative_data' + src] = []\n",
    "        output['relative_data' + tgt] = []\n",
    "    \n",
    "        for name in other_cols.keys():\n",
    "            output[name + src] = []\n",
    "            output[name + tgt] = []\n",
    "\n",
    "        for i in range(len(dataset) - bounds):\n",
    "            \n",
    "            output['categorical_data' + src].append(\n",
    "                categorical_data[i:i+context_sequence_length].values\n",
    "            )\n",
    "\n",
    "            output['categorical_data' + tgt].append(\n",
    "                categorical_data[i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "            )\n",
    "\n",
    "            relative_source = relative_data[i:i+context_sequence_length].values\n",
    "\n",
    "            relative_target = relative_data[i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "\n",
    "            relative_source = relative_source - relative_source[0]\n",
    "            relative_target = relative_target - relative_source[0]\n",
    "\n",
    "            output['relative_data' + src].append(\n",
    "                relative_source\n",
    "            )\n",
    "\n",
    "            output['relative_data' + tgt].append(\n",
    "                relative_target\n",
    "            )\n",
    "            \n",
    "\n",
    "            for name in dataset_groups.keys():\n",
    "                output[name + src].append(\n",
    "                    dataset_groups[name][i:i+context_sequence_length].values\n",
    "                )\n",
    "\n",
    "                output[name + tgt].append(\n",
    "                    dataset_groups[name][i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "                )\n",
    "\n",
    "        for name in output.keys():\n",
    "            if name == 'categorical_data' + src or name == 'categorical_data' + tgt:\n",
    "                output[name] = torch.tensor(output[name], dtype=torch.long, device=self.device)\n",
    "            else:\n",
    "                output[name] = torch.tensor(output[name], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1/11: We now look at ../data/01-12/DrDoS_NTP.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ../data/01-12/DrDoS_NTP.csv\n",
      "\tTo Dataset Cache: ./cache/DrDoS_NTP.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t../data/01-12/DrDoS_NTP.csv  \n",
      "        Job Number:\t\t\t1\n",
      "        Shape:\t\t\t\t(1217007, 88)\n",
      "        Samples:\t\t\t1217007 \n",
      "        Features:\t\t\t88\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "dataset_1 = examine_dataset(1, ddos_datasets_1, ddos_data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the preprocessor, we must load and clean our dataset. \n",
    "\n",
    "Since this is a prototype, we only load 1 of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting 7015 rows with Infinity in column Flow Bytes/s\n"
     ]
    }
   ],
   "source": [
    "ddos: pd.DataFrame = remove_infs_and_nans(dataset_1)\n",
    "ddos.columns = [column.strip() for column in ddos.columns]\n",
    "\n",
    "timestamps = ddos['Timestamp']\n",
    "std_time = timestamps.apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f').timestamp())\n",
    "ddos['Timestamp'] = std_time\n",
    "\n",
    "ddos = ddos.sort_values(by=['Timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell contains all of the groups of columns. Each group will be used in a slightly different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns are accounted for\n"
     ]
    }
   ],
   "source": [
    "time_cols= [\n",
    "    'Timestamp',\n",
    "]\n",
    "\n",
    "speed_cols = [\n",
    "    'Flow Bytes/s',\n",
    "\n",
    "    'Bwd Packets/s',\n",
    "    'Fwd Packets/s',\n",
    "    'Flow Packets/s',\n",
    "    \n",
    "    'Down/Up Ratio',\n",
    "]\n",
    "\n",
    "length_cols = [\n",
    "    'ACK Flag Count',\n",
    "    'CWE Flag Count',\n",
    "    'ECE Flag Count',\n",
    "    'FIN Flag Count',\n",
    "    'PSH Flag Count',\n",
    "    'RST Flag Count',\n",
    "    'SYN Flag Count',\n",
    "    'URG Flag Count',\n",
    "    \n",
    "    'Flow Duration',\n",
    "\n",
    "    'Bwd PSH Flags',\n",
    "    'Fwd PSH Flags',\n",
    "    'Bwd URG Flags',\n",
    "    'Fwd URG Flags',\n",
    "\n",
    "    'Bwd Header Length',\n",
    "    \n",
    "    'Total Length of Fwd Packets',\n",
    "    'Total Length of Bwd Packets',\n",
    "    'Fwd Header Length',\n",
    "    \n",
    "    'Total Backward Packets',\n",
    "    'Total Fwd Packets',\n",
    "    'Subflow Bwd Packets',\n",
    "    'Subflow Fwd Packets',\n",
    "    \n",
    "    'Subflow Fwd Bytes',\n",
    "    'Subflow Bwd Bytes',\n",
    "\n",
    "    'Bwd IAT Total',\n",
    "    'Fwd IAT Total',\n",
    "\n",
    "    'act_data_pkt_fwd',\n",
    "    'Init_Win_bytes_backward',\n",
    "    'Init_Win_bytes_forward',\n",
    "]\n",
    "\n",
    "categorical_cols = {\n",
    "    # column: number_of_possible_classes\n",
    "    'Protocol': 256,\n",
    "    'Inbound': 2,\n",
    "}\n",
    "\n",
    "min_cols = [\n",
    "    'Active Min',\n",
    "    'Idle Min',\n",
    "\n",
    "    'Bwd IAT Min',\n",
    "    'Fwd IAT Min',\n",
    "    'Flow IAT Min',\n",
    "    \n",
    "    'Bwd Packet Length Min',\n",
    "    'Fwd Packet Length Min',\n",
    "    'Min Packet Length',\n",
    "\n",
    "    'min_seg_size_forward',\n",
    "]\n",
    "\n",
    "max_cols = [\n",
    "    'Active Max',\n",
    "    'Idle Max',\n",
    "\n",
    "    'Bwd IAT Max',\n",
    "    'Fwd IAT Max',\n",
    "    'Flow IAT Max',\n",
    "    \n",
    "    'Max Packet Length',\n",
    "    'Bwd Packet Length Max',\n",
    "    'Fwd Packet Length Max',\n",
    "]\n",
    "\n",
    "mean_cols = [\n",
    "    'Active Mean',\n",
    "    'Idle Mean',\n",
    "\n",
    "    'Bwd Avg Bulk Rate',\n",
    "    'Fwd Avg Bulk Rate',\n",
    "    \n",
    "    'Bwd Avg Bytes/Bulk',\n",
    "    'Fwd Avg Bytes/Bulk',\n",
    "    \n",
    "    'Bwd IAT Mean',\n",
    "    'Fwd IAT Mean',\n",
    "    'Flow IAT Mean',\n",
    "    \n",
    "    'Packet Length Mean',\n",
    "    'Bwd Packet Length Mean',\n",
    "    'Fwd Packet Length Mean',\n",
    "    \n",
    "    'Average Packet Size',\n",
    "    'Bwd Avg Packets/Bulk',\n",
    "    'Fwd Avg Packets/Bulk',\n",
    "    \n",
    "    'Avg Bwd Segment Size',\n",
    "    'Avg Fwd Segment Size',\n",
    "]\n",
    "\n",
    "std_cols = [\n",
    "    'Active Std',\n",
    "    'Idle Std',\n",
    "    \n",
    "    'Bwd IAT Std',\n",
    "    'Fwd IAT Std',\n",
    "    'Flow IAT Std',\n",
    "\n",
    "    'Packet Length Std',\n",
    "    'Packet Length Variance',\n",
    "    'Bwd Packet Length Std',\n",
    "    'Fwd Packet Length Std',\n",
    "]\n",
    "\n",
    "port_cols = [\n",
    "    'Destination Port',\n",
    "    'Source Port',\n",
    "]\n",
    "\n",
    "unused_cols = [\n",
    "    'Unnamed: 0',\n",
    "    'Flow ID',\n",
    "    'Source IP',\n",
    "    'Destination IP',\n",
    "    'SimillarHTTP',\n",
    "    'Label',\n",
    "    'Fwd Header Length.1'\n",
    "] + port_cols\n",
    "\n",
    "# concatenate all column groups together to get a list of all columns\n",
    "all_cols = speed_cols + port_cols + length_cols + time_cols + list(categorical_cols.keys()) + min_cols + max_cols + mean_cols + std_cols + unused_cols\n",
    "\n",
    "# all_cols = time_cols + categorical_cols + min_cols + max_cols + mean_cols + std_cols + length_cols + unused_cols\n",
    "\n",
    " \n",
    "if set(ddos.columns) - set(all_cols) != set():\n",
    "    print('There are unaccounted for columns')\n",
    "else:\n",
    "    print('All columns are accounted for')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_groups = {\n",
    "    'speed_cols': speed_cols,\n",
    "    'length_cols': length_cols,\n",
    "    'min_cols': min_cols,\n",
    "    'max_cols': max_cols,\n",
    "    'mean_cols': mean_cols,\n",
    "    'std_cols': std_cols,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddos_clean = ddos.copy().drop(columns=unused_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, now that we have a clean dataset and we know how we are grouping the columns, we can use the preprocessor on the data and create a dataset.\n",
    "\n",
    "The datasets will consist of context (or source) and target sequences. The offset determines the starting index of the target sequence relative to the context sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    map_index_key = '__map_index__'\n",
    "    category_map = {}\n",
    "    old_maps = []\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline: List[Callable[[pd.DataFrame], pd.DataFrame]],\n",
    "        device: torch.device,\n",
    "    ) -> None:\n",
    "\n",
    "        self.pipeline = pipeline\n",
    "        self.device = device\n",
    "\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, leave_out: List[str] = ['Timestamp']):\n",
    "\n",
    "        X = X.copy()\n",
    "        for col in leave_out:\n",
    "            if col in X.columns:\n",
    "                X = X.drop(columns=[col])\n",
    "\n",
    "        for step in self.pipeline:\n",
    "            step.fit(X)\n",
    "\n",
    "        self.fitted = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(\n",
    "        self, \n",
    "        df: pd.DataFrame,\n",
    "        leave_out: List[str] = ['Timestamp']\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        if not self.fitted:\n",
    "            raise Exception('E1: You must fit the preprocessor before transforming')\n",
    "    \n",
    "        X = df.copy()\n",
    "        df = df.copy()\n",
    "\n",
    "        if leave_out != []:\n",
    "            # print(leave_out)\n",
    "            left_out = X[leave_out].copy()\n",
    "            X = X.drop(columns=leave_out)\n",
    "\n",
    "        columns: list = X.columns\n",
    "\n",
    "        for col in leave_out:\n",
    "            if col in columns:\n",
    "                raise Exception('E2: Undesired column %s was found in the transformed dataset' % col)\n",
    "\n",
    "        for step in self.pipeline:\n",
    "            X: np.ndarray = step.transform(X)\n",
    "\n",
    "        X = pd.DataFrame(X, columns=columns)\n",
    "        # df[columns].values = X\n",
    "\n",
    "        # df = pd.DataFrame(np.concatenate(X, left_out.values), columns = columns.extend(leave_out))\n",
    "\n",
    "        # if leave_out != []:\n",
    "        #     for col in leave_out:\n",
    "        #         df[col] = left_out[col]\n",
    "        #         # X[col] = left_out[col]\n",
    "        #     # X[leave_out] = left_out\n",
    "\n",
    "        for col in columns:\n",
    "            df[col] = X[col]\n",
    "\n",
    "        for col in leave_out:\n",
    "            if col not in df.columns:\n",
    "                raise Exception('E3: Column %s was not found in the transformed dataset' % col)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def index_categories(\n",
    "        self, \n",
    "        X: pd.DataFrame, \n",
    "        categorical_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        '''\n",
    "            We need to index the categorical columns so that they are in the range [0, n_categories) and save the mapping\n",
    "        '''\n",
    "        X = X.copy()\n",
    "\n",
    "        if self.map_index_key in categorical_cols:\n",
    "            raise Exception('Cannot use the reserved key %s as a column name' % self.map_index_key)\n",
    "\n",
    "        if self.category_map != {}:\n",
    "            self.old_maps.append(self.category_map)\n",
    "        \n",
    "        old_mapping_index = len(self.old_maps)\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            X[col], self.category_map[col] = pd.factorize(X[col])\n",
    "            self.category_map[self.map_index_key] = old_mapping_index    \n",
    "\n",
    "        return X\n",
    "\n",
    "    def create_dataset(\n",
    "        self, \n",
    "        dataset: pd.DataFrame,\n",
    "        categorical_cols: List[str],\n",
    "        relative_cols: List[str],\n",
    "        other_cols: Dict[str, List[str]],\n",
    "        context_sequence_length: int = 1,\n",
    "        target_sequence_length: int = 1,\n",
    "        target_sequence_offset: int = 0,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        '''\n",
    "            We create a dataset for each column group where each shares the same index\n",
    "            in the first dimension\n",
    "\n",
    "            This is because the groups will have different lengths in the last dimension\n",
    "            \n",
    "\n",
    "            We also need to create a context and target sequence for each column group\n",
    "                where the context sequence is the input and the target sequence is the output\n",
    "            \n",
    "                for each sample s in the dataset:\n",
    "                    context_sequence = df[s:s+context_sequence_length]\n",
    "                    target_sequence = df[s+target_sequence_offset:s+target_sequence_offset+target_sequence_length]\n",
    "\n",
    "\n",
    "        '''\n",
    "        \n",
    "        output = {}\n",
    "\n",
    "        src = '_source'\n",
    "        tgt = '_target'\n",
    "\n",
    "        if not self.fitted:\n",
    "            self.fit(\n",
    "                dataset, \n",
    "                leave_out=relative_cols + categorical_cols\n",
    "            )\n",
    "\n",
    "        categorical_data = dataset[categorical_cols]\n",
    "        relative_data = dataset[relative_cols]\n",
    "        # print(categorical_data[100:])\n",
    "\n",
    "        dataset = self.transform(\n",
    "            dataset,\n",
    "            leave_out=relative_cols + categorical_cols\n",
    "        )\n",
    "\n",
    "        # categorical_data = dataset[categorical_cols]\n",
    "        # print(dataset[100:])\n",
    "        # print(categorical_data[100:])\n",
    "        \n",
    "        bounds = max(\n",
    "            context_sequence_length, \n",
    "            target_sequence_length + target_sequence_offset\n",
    "        )\n",
    "\n",
    "        dataset_groups = {}\n",
    "\n",
    "        for i, (name, group) in enumerate(other_cols.items()):\n",
    "            dataset_groups[name] = dataset[group]\n",
    "\n",
    "        # categorical_data = dataset[categorical_cols]\n",
    "        # print(categorical_data[100:])\n",
    "        # categorical_data = self.index_categories(categorical_data, categorical_cols)\n",
    "        # print(categorical_data[100:])\n",
    "\n",
    "\n",
    "        output['categorical_data' + src] = []\n",
    "        output['categorical_data' + tgt] = []\n",
    "        output['relative_data' + src] = []\n",
    "        output['relative_data' + tgt] = []\n",
    "    \n",
    "        for name in other_cols.keys():\n",
    "            output[name + src] = []\n",
    "            output[name + tgt] = []\n",
    "\n",
    "        for i in range(len(dataset) - bounds):\n",
    "            \n",
    "            output['categorical_data' + src].append(\n",
    "                categorical_data[i:i+context_sequence_length].values\n",
    "            )\n",
    "\n",
    "            output['categorical_data' + tgt].append(\n",
    "                categorical_data[i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "            )\n",
    "\n",
    "            relative_source = relative_data[i:i+context_sequence_length].values\n",
    "\n",
    "            relative_target = relative_data[i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "\n",
    "            relative_source = relative_source - relative_source[0]\n",
    "            relative_target = relative_target - relative_source[0]\n",
    "\n",
    "            output['relative_data' + src].append(\n",
    "                relative_source\n",
    "            )\n",
    "\n",
    "            output['relative_data' + tgt].append(\n",
    "                relative_target\n",
    "            )\n",
    "            \n",
    "\n",
    "            for name in dataset_groups.keys():\n",
    "                output[name + src].append(\n",
    "                    dataset_groups[name][i:i+context_sequence_length].values\n",
    "                )\n",
    "\n",
    "                output[name + tgt].append(\n",
    "                    dataset_groups[name][i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "                )\n",
    "\n",
    "        for name in output.keys():\n",
    "            if name == 'categorical_data' + src or name == 'categorical_data' + tgt:\n",
    "                # print(\"asdf\", output[name][-1])\n",
    "                # print(output[name])\n",
    "                output[name] = torch.tensor(output[name], dtype=torch.long, device=self.device)\n",
    "                # print(output[name][-1])\n",
    "            else:\n",
    "                output[name] = torch.tensor(output[name], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gc/jb6dhmdd3r37x6bvx8rw9w980000gn/T/ipykernel_95232/1605220758.py:216: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343686209/work/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  output[name] = torch.tensor(output[name], dtype=torch.long, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_data_source torch.Size([49989, 10, 2])\n",
      "categorical_data_target torch.Size([49989, 3, 2])\n",
      "relative_data_source torch.Size([49989, 10, 1])\n",
      "relative_data_target torch.Size([49989, 3, 1])\n",
      "speed_cols_source torch.Size([49989, 10, 5])\n",
      "speed_cols_target torch.Size([49989, 3, 5])\n",
      "length_cols_source torch.Size([49989, 10, 28])\n",
      "length_cols_target torch.Size([49989, 3, 28])\n",
      "min_cols_source torch.Size([49989, 10, 9])\n",
      "min_cols_target torch.Size([49989, 3, 9])\n",
      "max_cols_source torch.Size([49989, 10, 8])\n",
      "max_cols_target torch.Size([49989, 3, 8])\n",
      "mean_cols_source torch.Size([49989, 10, 17])\n",
      "mean_cols_target torch.Size([49989, 3, 17])\n",
      "std_cols_source torch.Size([49989, 10, 9])\n",
      "std_cols_target torch.Size([49989, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "process = Preprocessor([StandardScaler()], device)\n",
    "\n",
    "context_length = 10\n",
    "target_length = 3\n",
    "target_offset = 8\n",
    "\n",
    "data_dict = process.create_dataset(\n",
    "    ddos_clean[:50000], # we only select 10000 samples for now\n",
    "    list(categorical_cols.keys()),\n",
    "    time_cols,\n",
    "    column_groups,\n",
    "    context_sequence_length=context_length,\n",
    "    target_sequence_length=target_length,\n",
    "    target_sequence_offset=target_offset\n",
    ")\n",
    "\n",
    "for name, tensor in data_dict.items():\n",
    "    print(name, tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the dataset for consumption by our model by wrapping it in a handy torch container and splitting it into train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we use the TensorDataset function to wrap all of the different parts of dataset. \n",
    "# The first dimension must be the same size across all inputs\n",
    "tensor_dataset = data.TensorDataset(\n",
    "    data_dict['relative_data_source'],\n",
    "    data_dict['categorical_data_source'],\n",
    "    data_dict['speed_cols_source'],\n",
    "    data_dict['length_cols_source'],\n",
    "    data_dict['min_cols_source'],\n",
    "    data_dict['max_cols_source'],\n",
    "    data_dict['mean_cols_source'],\n",
    "    data_dict['std_cols_source'],\n",
    "    data_dict['relative_data_target'],\n",
    "    data_dict['categorical_data_target'],\n",
    "    data_dict['speed_cols_target'],\n",
    "    data_dict['length_cols_target'],\n",
    "    data_dict['min_cols_target'],\n",
    "    data_dict['max_cols_target'],\n",
    "    data_dict['mean_cols_target'],\n",
    "    data_dict['std_cols_target'],\n",
    ")\n",
    "\n",
    "\n",
    "# We can now randomly split off a train and test dataset from the TensorDataset\n",
    "train_size = int(len(tensor_dataset) * 0.8)\n",
    "test_size = len(tensor_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(tensor_dataset, [train_size, test_size])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a batch size, we create our dataloaders. These will randomize the minibatches for use by the model during each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = data.DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "Now that we have a dataset and we understand its structure, we can begin to build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        p: float = .05,\n",
    "    ):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, embeddings: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # print(f\"training: {self.training}\")\n",
    "\n",
    "        if not self.training:\n",
    "            return sum(embeddings)\n",
    "\n",
    "        else:\n",
    "            mask = torch.rand(\n",
    "                embeddings[0].shape[0],\n",
    "                embeddings[0].shape[1],\n",
    "                len(embeddings), \n",
    "                1, \n",
    "                device=embeddings[0].device\n",
    "            ) < self.p\n",
    "\n",
    "            mask = mask.float()\n",
    "\n",
    "            return sum([mask[:, :, i, :] * emb for i, emb in enumerate(embeddings)])\n",
    "\n",
    "\n",
    "class RandomFourierTimeEncoding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        input_size: int = 1,\n",
    "        encoding_half_size: int = 50,\n",
    "        sigma: float = 4,\n",
    "    ) -> None:\n",
    "        super(RandomFourierTimeEncoding, self).__init__()\n",
    "\n",
    "        self.fourier = GaussianEncoding(\n",
    "            sigma=sigma,\n",
    "            input_size=input_size,\n",
    "            encoded_size=encoding_half_size,\n",
    "        ).to(device)\n",
    "        # )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "            X: (batch_size, sequence_length, input_size)\n",
    "            output: (batch_size, sequence_length, 2*encoding_half_size + input_size)\n",
    "        '''\n",
    "\n",
    "        output = self.fourier(X)\n",
    "        output = torch.cat([X, output], dim=-1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generative Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.5783, -0.1387,  0.3966,  0.4295, -0.1705],\n",
       "          [ 0.7375, -0.1257,  0.2527,  0.4441, -0.2979],\n",
       "          [ 0.4540, -0.0599,  0.3966,  0.4295, -0.1705],\n",
       "          [ 0.6783, -0.1749,  0.3205,  0.4287, -0.2359]],\n",
       " \n",
       "         [[ 0.5745,  0.0079,  0.2248,  0.3963, -0.3272],\n",
       "          [ 0.7106, -0.0327,  0.1319,  0.4714, -0.3979],\n",
       "          [ 0.6347, -0.0995,  0.2275,  0.4285, -0.3159],\n",
       "          [ 0.4117,  0.0329,  0.3412,  0.4585, -0.2250]],\n",
       " \n",
       "         [[ 0.6880, -0.0276,  0.1312,  0.4749, -0.4097],\n",
       "          [ 0.6300, -0.1601,  0.3411,  0.4724, -0.2283],\n",
       "          [ 0.7776, -0.0905,  0.3601,  0.2448, -0.2100],\n",
       "          [ 0.6020, -0.0151,  0.2562,  0.4510, -0.2429]]],\n",
       "        grad_fn=<CatBackward0>),\n",
       " tensor([[[0.4122, 0.5878, 0.2550, 0.3573, 0.3877],\n",
       "          [0.4102, 0.5898, 0.2083, 0.3832, 0.4086],\n",
       "          [0.4120, 0.5880, 0.2690, 0.3497, 0.3813],\n",
       "          [0.4104, 0.5896, 0.1897, 0.4091, 0.4012]],\n",
       " \n",
       "         [[0.4105, 0.5895, 0.2046, 0.4017, 0.3938],\n",
       "          [0.4091, 0.5909, 0.1898, 0.4103, 0.3999],\n",
       "          [0.4117, 0.5883, 0.2129, 0.3806, 0.4065],\n",
       "          [0.4115, 0.5885, 0.2541, 0.3659, 0.3800]],\n",
       " \n",
       "         [[0.4092, 0.5908, 0.1858, 0.4163, 0.3980],\n",
       "          [0.4104, 0.5896, 0.2098, 0.3902, 0.4000],\n",
       "          [0.4098, 0.5902, 0.1982, 0.3989, 0.4028],\n",
       "          [0.4098, 0.5902, 0.2323, 0.3773, 0.3905]]], grad_fn=<CatBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrafficGenerativeHead(nn.Module):\n",
    "    '''\n",
    "        This is the head of the decoder.\n",
    "\n",
    "        params:\n",
    "            input_size: the size of the last dimension of the input\n",
    "            categorical_cols: a dictionary of column names and the number of categories in each column\n",
    "            column_groups: a dictionary of column groups, each with a group name a list of column names\n",
    "            \n",
    "\n",
    "        For each categorical column, we create a classification trunk of layers that will end in a softmax layer of size num_categories\n",
    "        For each column group, we create a regression trunk of layers that will end in a linear layer that is the length of the column group\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int, \n",
    "        categorical_cols: Dict[str, int],\n",
    "        column_groups: Dict[str, List[str]],\n",
    "        device: torch.device,\n",
    "    ) -> None:\n",
    "    \n",
    "        super(TrafficGenerativeHead, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.column_groups = column_groups\n",
    "        self.device = device\n",
    "\n",
    "        self.categorical_head = nn.ModuleDict()\n",
    "\n",
    "        for column, num_categories in categorical_cols.items():\n",
    "            hidden_size = (input_size + num_categories) // 2\n",
    "            self.categorical_head[column] = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size, device = device),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, num_categories, device = device),\n",
    "                nn.Softmax(dim=-1)\n",
    "            )\n",
    "\n",
    "        self.continuous_head = nn.ModuleDict()\n",
    "\n",
    "        for column_group, columns in column_groups.items():\n",
    "            hidden_size = (input_size + len(columns)) // 2\n",
    "            self.continuous_head[column_group] = nn.Sequential(\n",
    "                nn.Linear(input_size, hidden_size, device = device),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, len(columns), device = device),\n",
    "            )\n",
    "\n",
    "        # self.inspector = DebugLayer()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = self.inspector(x)\n",
    "\n",
    "        # print(f\"generative head input shape: {x.shape}\")\n",
    "\n",
    "        cont_output = torch.cat(\n",
    "            [head(x) for head in self.continuous_head.values()],\n",
    "            dim= -1\n",
    "        )\n",
    "\n",
    "        cat_output = torch.cat(\n",
    "            [head(x) for head in self.categorical_head.values()],\n",
    "            dim= -1\n",
    "        )\n",
    "\n",
    "        return cont_output, cat_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TESTING\n",
    "# Here we test the generative head using some dummy data\n",
    "\n",
    "\n",
    "batch_size: int = 3\n",
    "samples: int    = 4\n",
    "variables: int  = 5\n",
    "\n",
    "column_groups = {\n",
    "    'sample_group_1':[\n",
    "        'col_1',\n",
    "        'col_2',\n",
    "    ],\n",
    "    'sample_group_2': [\n",
    "        'col_3',\n",
    "        'col_4',\n",
    "        'col_5',\n",
    "    ],\n",
    "}\n",
    "\n",
    "categorical_columns = {\n",
    "    'cat_1': 2,\n",
    "    'cat_2': 3,\n",
    "}\n",
    "\n",
    "test_vals = torch.rand(batch_size, samples, variables)\n",
    "\n",
    "\n",
    "model = TrafficGenerativeHead(variables, categorical_columns, column_groups, device)\n",
    "model(test_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [-inf, 0., -inf, -inf, -inf, -inf],\n",
       "        [-inf, 0., 0., -inf, -inf, -inf],\n",
       "        [-inf, 0., -inf, 0., -inf, -inf],\n",
       "        [-inf, 0., -inf, 0., 0., -inf],\n",
       "        [-inf, 0., -inf, 0., -inf, 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrafficFlowTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_cols: Dict[str, int],\n",
    "        column_groups: Dict[str, List[str]],\n",
    "        device: torch.device,\n",
    "        dropout_rate: float = .1,\n",
    "        embedding_size: int = 128,\n",
    "        encoding_size: int = 50,\n",
    "        encoder_heads: int = 8,\n",
    "        encoder_layers: int = 2,\n",
    "        encoder_forward_expansion: int = 2,\n",
    "        decoder_heads: int = 8,\n",
    "        decoder_layers: int = 2,\n",
    "        decoder_forward_expansion: int = 2\n",
    "    ) -> None:\n",
    "    \n",
    "        super(TrafficFlowTransformer, self).__init__()\n",
    "    \n",
    "        self.categorical_cols = categorical_cols\n",
    "\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            RandomFourierTimeEncoding(\n",
    "                device=device,\n",
    "                input_size=1,\n",
    "                encoding_half_size=encoding_size,\n",
    "            ),\n",
    "            nn.Linear(2*encoding_size + 1, embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "        ).to(device)\n",
    "\n",
    "        self.categorical_embeddings = nn.ModuleDict({\n",
    "            col: nn.Embedding(num_embeddings=n_categories, embedding_dim=embedding_size)\n",
    "            for col, n_categories in categorical_cols.items()\n",
    "        }).to(device)\n",
    "\n",
    "        self.continuous_embeddings = nn.ModuleDict({\n",
    "            group: nn.Sequential(\n",
    "                nn.Linear(len(cols), embedding_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(embedding_size, embedding_size),\n",
    "                nn.ReLU(),\n",
    "            ) for group, cols in column_groups.items()\n",
    "        }).to(device)\n",
    "\n",
    "        self.emb_dropout = EmbeddingDropout()\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model            = embedding_size,\n",
    "            nhead              = encoder_heads,\n",
    "            num_encoder_layers = encoder_layers,\n",
    "            num_decoder_layers = decoder_layers,\n",
    "            dim_feedforward    = encoder_forward_expansion*embedding_size,\n",
    "            dropout            = dropout_rate,\n",
    "            batch_first        = True,\n",
    "        ).to(device)\n",
    "\n",
    "        self.generative_head = TrafficGenerativeHead(\n",
    "            embedding_size,\n",
    "            categorical_cols,\n",
    "            column_groups,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def source_embedding(\n",
    "        self, \n",
    "        X_time      : torch.Tensor, \n",
    "        X_cat       : torch.Tensor, \n",
    "        X_cont_group: Dict[str, torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "            This method takes the input source data and embeds it using RFF for the temporal features,\n",
    "                an embedding layer for the categorical features, and a linear layer for the continuous features\n",
    "\n",
    "            Each input is a sequence and this produces a sequence of embeddings with a 1:1 mapping between elements in the sequence\n",
    "\n",
    "            We perform Embedding dropout during training before summing up the individual embeddings to create our input embeddings\n",
    "\n",
    "            Input shapes:\n",
    "                X_time: (batch_size, sequence_length, 1)\n",
    "                X_cat: (batch_size, sequence_length, n_categorical_cols)\n",
    "                X_cont_group: Dict[torch.Tensor] where each tensor has shape (batch_size, sequence_length, m_continuous_cols) \n",
    "                                    where m is independent for each tensor in the dict\n",
    "        \n",
    "            Output shape:\n",
    "                X_emb: (batch_size, sequence_length, embedding_size)\n",
    "\n",
    "        '''\n",
    "\n",
    "        X_time_emb = self.time_embedding(X_time)\n",
    "        X_cat_embs = [\n",
    "            self.categorical_embeddings[col](X_cat[:, :, i])\n",
    "            for i, col in enumerate(self.categorical_cols.keys())\n",
    "        ]\n",
    "        X_cont_embs = [\n",
    "            self.continuous_embeddings[group](X_cont_group[group])\n",
    "            for group in self.continuous_embeddings.keys()\n",
    "        ]\n",
    "\n",
    "        X_emb = self.emb_dropout(X_cat_embs + X_cont_embs + [X_time_emb])\n",
    "\n",
    "        return X_emb\n",
    "\n",
    "\n",
    "    def target_embedding(\n",
    "        self, \n",
    "        y_time      : torch.Tensor, \n",
    "        y_cat       : torch.Tensor, \n",
    "        y_cont_group: Dict[str, torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "            This method takes the input target data and embeds it using RFF for the temporal features,\n",
    "                an embedding layer for the categorical features, and a linear layer for the continuous features\n",
    "\n",
    "            Each input is a sequence and this produces a sequence of embeddings with a 2:1 mapping between elements in the sequence\n",
    "                where the first element is only the temporal embedding, and the second element is the full target embedding. \n",
    "\n",
    "            This is because we structure the input to the decoder as [Query, Response] where the query is the temporal embedding\n",
    "                and the response is the full target embedding. We want the model to predict what the full target is given the position\n",
    "                in time.\n",
    "\n",
    "            There is no embedding dropout performed here and within the full target embedding, the embeddings are simply summed up\n",
    "\n",
    "            Input shapes:\n",
    "                y_time: (batch_size, sequence_length, 1)\n",
    "                y_cat: (batch_size, sequence_length, n_categorical_cols)\n",
    "                y_cont_group: Dict[torch.Tensor] where each tensor has shape (batch_size, sequence_length, m_continuous_cols)\n",
    "                                    where m is independent for each tensor in the dict\n",
    "\n",
    "            Output shape:\n",
    "                y_time_emb: (batch_size, 2 * sequence_length, embedding_size)\n",
    "        '''\n",
    "\n",
    "\n",
    "        y_time_emb = self.time_embedding(y_time)\n",
    "        y_cat_embs = [\n",
    "            self.categorical_embeddings[col](y_cat[:, :, i])\n",
    "            for i, col in enumerate(self.categorical_cols.keys())\n",
    "        ]\n",
    "        y_cont_embs = [\n",
    "            self.continuous_embeddings[group](y_cont_group[group])\n",
    "            for group in self.continuous_embeddings.keys()\n",
    "        ]\n",
    "\n",
    "        y_full_target_emb = y_time_emb + sum(y_cat_embs) + sum(y_cont_embs)        \n",
    "\n",
    "        # print(f\"y_time_emb: {y_time_emb.shape}\")\n",
    "        # print(f\"y_full_target_emb: {y_full_target_emb.shape}\")\n",
    "\n",
    "        # we repeat the time embedding for each element in the target sequence\n",
    "\n",
    "        b, s, e = y_full_target_emb.shape\n",
    "\n",
    "        y_time_emb = y_time_emb.reshape(\n",
    "            b, s, 1, e\n",
    "        ).repeat(\n",
    "            1, 1, 2, 1\n",
    "        ).reshape(\n",
    "            b, 2*s, e\n",
    "        )       \n",
    "\n",
    "        y_emb = y_time_emb\n",
    "\n",
    "        y_emb[:, 1::2, :] = y_full_target_emb\n",
    "\n",
    "        # brief test to make sure that the embeddings are being created correctly\n",
    "\n",
    "        # check that the first element of y_emb comes from y_time_emb\n",
    "        assert torch.all(torch.eq(y_emb[:, 0, :], y_time_emb[:, 0, :]))\n",
    "\n",
    "        # check that every other element from y_emb comes from y_full_target_emb\n",
    "        for i in range(1, y_emb.shape[1], 2):\n",
    "            assert torch.all(torch.eq(y_emb[:, i, :], y_full_target_emb[:, i//2, :]))\n",
    "\n",
    "        return y_emb\n",
    "\n",
    "\n",
    "    def target_mask(self, sequence_length: int) -> torch.Tensor:\n",
    "        '''\n",
    "            This method generates our target mask for the decoder\n",
    "\n",
    "            The mask is additive and has shape (2 * sequence_length, 2 * sequence_length)\n",
    "\n",
    "            The mask will make sure the query attends only to previous responses and not future responses\n",
    "                The query also attends to itself, but not to other queries\n",
    "        '''\n",
    "\n",
    "        mask = torch.triu(torch.ones(2*sequence_length, 2*sequence_length), diagonal=1)\n",
    "\n",
    "        # now we need to make sure that the queries only attend to itself and previous responses by masking all other queries\n",
    "\n",
    "        for i in range(0, 2 * sequence_length - 1, 2):\n",
    "            mask[:, i] = 1\n",
    "            mask[i, i] = 0\n",
    "\n",
    "        return mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "\n",
    "    def old_forward(\n",
    "        self,\n",
    "        X_time      : torch.Tensor,\n",
    "        X_cat       : torch.Tensor,\n",
    "        X_cont_group: Dict[str, torch.Tensor],\n",
    "        y_time      : torch.Tensor,\n",
    "        y_cat       : torch.Tensor,\n",
    "        y_cont_group: Dict[str, torch.Tensor]\n",
    "    ):\n",
    "\n",
    "        \n",
    "        print(f\"X_time: {X_time.shape}\")\n",
    "        print(f\"X_cat: {X_cat.shape}\")\n",
    "        # print(f\"X_cont_group: {X_cont_group.shape}\")\n",
    "        print(f\"X_cont_group: {len(X_cont_group)}\")\n",
    "        print(f\"y_time: {y_time.shape}\")\n",
    "\n",
    "        # we embed the time\n",
    "        X_time_emb = self.time_embedding(X_time)\n",
    "        y_time_emb = self.time_embedding(y_time)\n",
    "\n",
    "        print(f\"X_time_emb: {X_time_emb.shape}\")\n",
    "        print(f\"y_time_emb: {y_time_emb.shape}\")\n",
    "\n",
    "        # we embed the categorical columns\n",
    "        # X_cat_embs = [\n",
    "        #     self.categorical_embeddings[col](X_cat[:, :, i])\n",
    "        #     for i, col in enumerate(self.categorical_cols.keys())\n",
    "        # ]\n",
    "\n",
    "        print(self.categorical_embeddings)\n",
    "\n",
    "        X_cat_embs = []\n",
    "        for i, col in enumerate(self.categorical_cols.keys()):\n",
    "            print(i, col, X_cat.size(), X_cat[:, :, i])\n",
    "            X_cat_embs.append(self.categorical_embeddings[col](X_cat[:, :, i]))\n",
    "\n",
    "        print(*[f\"X_cat_embs[{i}]: {emb.shape}\" for i, emb in enumerate(X_cat_embs)], sep='\\n')\n",
    "\n",
    "        # we embed the continuous columns\n",
    "        X_cont_embs = [\n",
    "            self.continuous_embeddings[group](X_cont_group[group])\n",
    "            for group in self.continuous_embeddings.keys()\n",
    "        ]\n",
    "\n",
    "        y_cont_embs = [\n",
    "            self.continuous_embeddings[group](y_cont_group[group])\n",
    "            for group in self.continuous_embeddings.keys()\n",
    "        ]\n",
    "\n",
    "        print(*[f\"X_cont_embs[{i}]: {emb.shape}\" for i, emb in enumerate(X_cont_embs)], sep='\\n')\n",
    "        print(*[f\"y_cont_embs[{i}]: {emb.shape}\" for i, emb in enumerate(y_cont_embs)], sep='\\n')\n",
    "\n",
    "        # print(f\"X_cat_embs: {[emb.shape for emb in X_cat_embs]}\")\n",
    "        # print(f\"X_cont_embs: {[emb.shape for emb in X_cont_embs]}\")\n",
    "\n",
    "        print(f'training: {self.training}')\n",
    "\n",
    "        X_emb = self.emb_dropout(X_cat_embs + X_cont_embs + [X_time_emb])\n",
    "\n",
    "        # X_emb = X_time_emb + sum(X_cat_embs) + sum(X_cont_embs)\n",
    "\n",
    "        print(f\"X_emb: {X_emb.shape}\")\n",
    "\n",
    "        # # X_enc = self.encoder(X_emb)\n",
    "\n",
    "        # print(f\"X_enc: {X_enc.shape}\")\n",
    "\n",
    "        # X_fourier = torch.fft.fft(X_enc.to(torch.device('cpu')), dim=-1)\n",
    "\n",
    "        # print(f\"X_fourier: {X_fourier.shape}\")\n",
    "\n",
    "        return 1, 2\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X_time      : torch.Tensor,\n",
    "        X_cat       : torch.Tensor,\n",
    "        X_cont_group: Dict[str, torch.Tensor],\n",
    "        y_time      : torch.Tensor,\n",
    "        y_cat       : torch.Tensor,\n",
    "        y_cont_group: Dict[str, torch.Tensor]\n",
    "    ):\n",
    "        \n",
    "        target_sequence_length = y_time.shape[1]\n",
    "\n",
    "        X_emb = self.source_embedding(X_time, X_cat, X_cont_group)\n",
    "        y_emb = self.target_embedding(y_time, y_cat, y_cont_group)\n",
    "\n",
    "        # print(f\"X_emb: {X_emb.shape}\")\n",
    "        # print(f\"y_emb: {y_emb.shape}\")\n",
    "\n",
    "        pre_output = self.transformer(\n",
    "            X_emb,\n",
    "            y_emb,\n",
    "            tgt_mask=self.target_mask(target_sequence_length),\n",
    "        )\n",
    "\n",
    "        # print(f\"pre_output: {pre_output.shape}\")\n",
    "\n",
    "        cont_output, cat_output = self.generative_head(pre_output)\n",
    "\n",
    "        # print(f\"cont_output: {cont_output.shape}\")\n",
    "        # print(f\"cat_output: {cat_output.shape}\")\n",
    "\n",
    "        # return only the processed query embeddings and not the response embeddings\n",
    "\n",
    "        cont_output = cont_output[:, 0::2, :]\n",
    "        cat_output = cat_output[:, 0::2, :]\n",
    "\n",
    "        # print(f\"cont_output: {cont_output.shape}\")\n",
    "        # print(f\"cat_output: {cat_output.shape}\")\n",
    "\n",
    "        return cont_output, cat_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TESTING\n",
    "# we pass in a single mini-batch of data to test the forward pass\n",
    "\n",
    "\n",
    "n_epochs = 1\n",
    "\n",
    "column_groups = {\n",
    "    'speed_cols': speed_cols,\n",
    "    'length_cols': length_cols,\n",
    "    'min_cols': min_cols,\n",
    "    'max_cols': max_cols,\n",
    "    'mean_cols': mean_cols,\n",
    "    'std_cols': std_cols,\n",
    "}\n",
    "\n",
    "categorical_cols = {\n",
    "    'Protocol': 256,\n",
    "    'Inbound': 2,\n",
    "}\n",
    "\n",
    "epoch_length = len(train_dataloader.dataset)\n",
    "\n",
    "train_losses = []\n",
    "test_losses  = []\n",
    "train_counter = []\n",
    "test_counter = [i * epoch_length for i in range(n_epochs + 1)]\n",
    "\n",
    "\n",
    "batch_0 = None\n",
    "\n",
    "model = TrafficFlowTransformer(\n",
    "    categorical_cols,\n",
    "    column_groups,\n",
    "    embedding_size=128,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# print(model)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    # model.eval()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        X_time = batch[0]\n",
    "        y_time = batch[8]\n",
    "\n",
    "        X_cat = batch[1]\n",
    "        y_cat = batch[9]\n",
    "\n",
    "        X_cont_group = batch[2:8]\n",
    "        X_cont_group_dict = {\n",
    "            group: X_cont_group[i] for i, group in enumerate(column_groups.keys())\n",
    "        }\n",
    "\n",
    "\n",
    "        y_cont_group = batch[10:]\n",
    "        y_cont_group_dict = {\n",
    "            group: y_cont_group[i] for i, group in enumerate(column_groups.keys())\n",
    "        }\n",
    "        # print(X_cat.size())\n",
    "\n",
    "        y_cont_group, y_cat_pred = model(\n",
    "            X_time,\n",
    "            X_cat,\n",
    "            X_cont_group_dict,\n",
    "            y_time,\n",
    "            y_cat,\n",
    "            y_cont_group_dict\n",
    "        )\n",
    "\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "model.target_mask(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedLoss(nn.Module):\n",
    "    \"Measures how well we have generated the sequence item\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        categorical_cols,\n",
    "        cont_column_groups,\n",
    "        device\n",
    "    ):\n",
    "        super(GeneratedLoss, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.cont_column_groups = cont_column_groups\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        cat_preds,\n",
    "        cont_preds,\n",
    "        cat_targs, \n",
    "        cont_targs\n",
    "    ):\n",
    "\n",
    "        cats, conts = cat_preds, cont_preds\n",
    "        total_ce, total_mse = cats.new([0]), conts.new([0])\n",
    "        \n",
    "        # first we handle the categorical predictions\n",
    "        current = 0\n",
    "        num_categorical_cols = len(self.categorical_cols)\n",
    "        for i, (cat_feature, num_classes) in enumerate(self.categorical_cols.items()):\n",
    "            preds = cats[:, :, current:current+num_classes]\n",
    "            \n",
    "            b, s, e = preds.shape\n",
    "            targ = cat_targs[:, :, i].reshape(b * s)\n",
    "            preds = preds.reshape(b * s, e)\n",
    "\n",
    "\n",
    "            total_ce += self.ce(preds, targ)\n",
    "            current += num_classes\n",
    "\n",
    "        # We normalize the loss by the number of categorical columns\n",
    "        total_ce /= num_categorical_cols\n",
    "\n",
    "\n",
    "        # now we handle the continuous predictions\n",
    "\n",
    "        current = 0\n",
    "        # for i, (group, columns) in enumerate(self.cont_column_groups.items()):\n",
    "        #     print(f'group: {group}, columns: {len(columns)}')\n",
    "\n",
    "        for i in range(len(cont_targs)):\n",
    "            # print(f\"cont_targs[{i}]: {cont_targs[i].shape}\")\n",
    "            cols = cont_targs[i].shape[-1]\n",
    "            end = current + cols\n",
    "\n",
    "            preds = conts[:, :, current:end].reshape(b * s, cols)\n",
    "            targ = cont_targs[i].reshape(b * s, cols)\n",
    "            # print(f'preds: {preds.shape}') \n",
    "            # print(f'targ: {targ.shape}')\n",
    "            \n",
    "            total_mse += self.mse(preds, targ)\n",
    "\n",
    "            current = end\n",
    "        \n",
    "        # We normalize the loss by the number of continuous columns\n",
    "        total_mse /= len(cont_targs)\n",
    "\n",
    "\n",
    "        total = total_ce + total_mse\n",
    "        total /= s # we normalize the loss by the sequence length\n",
    "        total /= b # finally, we normalize the loss by the batch size\n",
    "\n",
    "        return total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 99\u001b[0m\n\u001b[1;32m     91\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(\n\u001b[1;32m     92\u001b[0m         y_cat_pred,\n\u001b[1;32m     93\u001b[0m         y_cont_pred,\n\u001b[1;32m     94\u001b[0m         y_cat,\n\u001b[1;32m     95\u001b[0m         y_cont_group\n\u001b[1;32m     96\u001b[0m     )\n\u001b[1;32m     98\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast_gpu/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast_gpu/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 100\n",
    "warmup_epochs = n_epochs // 10\n",
    "encoder_layers = 1\n",
    "decoder_layers = 1\n",
    "forward_expansion = 1\n",
    "heads = 4\n",
    "embedding_size = 96\n",
    "\n",
    "\n",
    "column_groups = {\n",
    "    'speed_cols': speed_cols,\n",
    "    'length_cols': length_cols,\n",
    "    'min_cols': min_cols,\n",
    "    'max_cols': max_cols,\n",
    "    'mean_cols': mean_cols,\n",
    "    'std_cols': std_cols,\n",
    "}\n",
    "\n",
    "categorical_cols = {\n",
    "    'Protocol': 256,\n",
    "    'Inbound': 2,\n",
    "}\n",
    "\n",
    "epoch_length = len(train_dataloader.dataset)\n",
    "\n",
    "train_losses = []\n",
    "test_losses  = []\n",
    "train_counter = []\n",
    "test_counter = [i * epoch_length for i in range(n_epochs + 1)]\n",
    "\n",
    "\n",
    "loss_fn = GeneratedLoss(\n",
    "    categorical_cols,\n",
    "    column_groups,\n",
    "    device\n",
    ")\n",
    "\n",
    "batch_0 = None\n",
    "\n",
    "model = TrafficFlowTransformer(\n",
    "    categorical_cols,\n",
    "    column_groups,\n",
    "    embedding_size=embedding_size,\n",
    "    encoder_layers=encoder_layers,\n",
    "    encoder_forward_expansion=forward_expansion,\n",
    "    encoder_heads=heads,\n",
    "    decoder_layers=decoder_layers,\n",
    "    decoder_forward_expansion=forward_expansion,\n",
    "    decoder_heads=heads,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optimizer,\n",
    "#     T_max=n_epochs,\n",
    "#     eta_min=0.0001\n",
    "# )\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    message = f\"Epoch: {epoch}, Train Loss:\"\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        X_time = batch[0]\n",
    "        y_time = batch[8]\n",
    "\n",
    "        X_cat = batch[1]\n",
    "        y_cat = batch[9]\n",
    "\n",
    "        X_cont_group = batch[2:8]\n",
    "        X_cont_group_dict = {\n",
    "            group: X_cont_group[i] for i, group in enumerate(column_groups.keys())\n",
    "        }\n",
    "\n",
    "\n",
    "        y_cont_group = batch[10:]\n",
    "        y_cont_group_dict = {\n",
    "            group: y_cont_group[i] for i, group in enumerate(column_groups.keys())\n",
    "        }\n",
    "        # print(X_cat.size())\n",
    "\n",
    "        y_cont_pred, y_cat_pred = model(\n",
    "            X_time,\n",
    "            X_cat,\n",
    "            X_cont_group_dict,\n",
    "            y_time,\n",
    "            y_cat,\n",
    "            y_cont_group_dict\n",
    "        )\n",
    "\n",
    "        loss = loss_fn(\n",
    "            y_cat_pred,\n",
    "            y_cont_pred,\n",
    "            y_cat,\n",
    "            y_cont_group\n",
    "        )\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # scheduler.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    message += f\" {train_loss}\"\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            for batch in test_dataloader:\n",
    "                X_time = batch[0]\n",
    "                y_time = batch[8]\n",
    "\n",
    "                X_cat = batch[1]\n",
    "                y_cat = batch[9]\n",
    "\n",
    "                X_cont_group = batch[2:8]\n",
    "                X_cont_group_dict = {\n",
    "                    group: X_cont_group[i] for i, group in enumerate(column_groups.keys())\n",
    "                }\n",
    "\n",
    "\n",
    "                y_cont_group = batch[10:]\n",
    "                y_cont_group_dict = {\n",
    "                    group: y_cont_group[i] for i, group in enumerate(column_groups.keys())\n",
    "                }\n",
    "\n",
    "                y_cont_pred, y_cat_pred = model(\n",
    "                    X_time,\n",
    "                    X_cat,\n",
    "                    X_cont_group_dict,\n",
    "                    y_time,\n",
    "                    y_cat,\n",
    "                    y_cont_group_dict\n",
    "                )\n",
    "\n",
    "                test_loss += loss_fn(\n",
    "                    y_cat_pred,\n",
    "                    y_cont_pred,\n",
    "                    y_cat,\n",
    "                    y_cont_group\n",
    "                )\n",
    "\n",
    "            test_loss /= len(test_dataloader.dataset)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "            message += f\", Test Loss: {test_loss}\"\n",
    "\n",
    "    print(message)\n",
    "\n",
    "# model.target_mask(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 1\n",
    "\n",
    "column_groups = {\n",
    "    'speed_cols': speed_cols,\n",
    "    'length_cols': length_cols,\n",
    "    'min_cols': min_cols,\n",
    "    'max_cols': max_cols,\n",
    "    'mean_cols': mean_cols,\n",
    "    'std_cols': std_cols,\n",
    "}\n",
    "\n",
    "categorical_cols = {\n",
    "    'Protocol': 256,\n",
    "    'Inbound': 2,\n",
    "}\n",
    "\n",
    "epoch_length = len(train_dataloader.dataset)\n",
    "\n",
    "train_losses = []\n",
    "test_losses  = []\n",
    "train_counter = []\n",
    "test_counter = [i * epoch_length for i in range(n_epochs + 1)]\n",
    "\n",
    "\n",
    "loss_fn = GeneratedLoss(\n",
    "    categorical_cols,\n",
    "    device\n",
    ")\n",
    "\n",
    "batch_0 = None\n",
    "\n",
    "model = TrafficFlowTransformer(\n",
    "    categorical_cols,\n",
    "    column_groups,\n",
    "    embedding_size=128,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# print(model)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    # model.eval()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        X_time = batch[0]\n",
    "        y_time = batch[8]\n",
    "\n",
    "        X_cat = batch[1]\n",
    "        y_cat = batch[9]\n",
    "\n",
    "        X_cont_group = batch[2:8]\n",
    "        X_cont_group_dict = {\n",
    "            group: X_cont_group[i] for i, group in enumerate(column_groups.keys())\n",
    "        }\n",
    "\n",
    "\n",
    "        y_cont_group = batch[10:]\n",
    "        y_cont_group_dict = {\n",
    "            group: y_cont_group[i] for i, group in enumerate(column_groups.keys())\n",
    "        }\n",
    "        # print(X_cat.size())\n",
    "\n",
    "        y_cat_pred, y_cont_group = model(\n",
    "            X_time,\n",
    "            X_cat,\n",
    "            X_cont_group_dict,\n",
    "            y_time,\n",
    "            y_cat,\n",
    "            y_cont_group_dict\n",
    "        )\n",
    "\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "model.target_mask(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "source_sequence_length = 10\n",
    "target_sequence_length = 3\n",
    "target_sequence_offset = 8\n",
    "\n",
    "embedding_size = 4\n",
    "\n",
    "test_time_vals = torch.arange(0, target_sequence_length).repeat(batch_size, embedding_size)\n",
    "test_full_targ_vals = torch.arange(target_sequence_length, 2 * target_sequence_length)\n",
    "print(test_time_vals.shape)\n",
    "print(test_time_vals.size(1))\n",
    "print(test_full_targ_vals.shape)\n",
    "print(test_time_vals)\n",
    "print(test_full_targ_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vals = torch.arange(0, target_sequence_length).reshape(1, target_sequence_length, 1).repeat(batch_size, 1, embedding_size)\n",
    "test_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vals = test_vals.reshape(batch_size, target_sequence_length, 1, embedding_size).repeat(1, 1, 2, 1).reshape(batch_size, target_sequence_length*2, embedding_size)\n",
    "test_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_vals = torch.arange(target_sequence_length, target_sequence_length * 2).reshape(1, target_sequence_length, 1).repeat(batch_size, 1, embedding_size)\n",
    "targ_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in test_vals, we replace every other element of the first dimension with an element from targ_vals\n",
    "test_vals[:, 1::2, :] = targ_vals\n",
    "test_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we remove all the elements that were replaced\n",
    "\n",
    "test_vals[:, 0::2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('fast_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deaf3a77c670ae4bcde0102b8754366ef6094e7b1a1d30917591bfafe6a4ef0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
