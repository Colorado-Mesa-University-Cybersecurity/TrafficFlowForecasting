{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "import os\n",
    "import pprint\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from datetime import (\n",
    "    datetime\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler\n",
    ")\n",
    "\n",
    "from typing import (\n",
    "    Callable,\n",
    "    Dict,\n",
    "    List,\n",
    "    Tuple\n",
    ")\n",
    "\n",
    "from Quick.cleaning.loading import (\n",
    "    examine_dataset,\n",
    "    remove_infs_and_nans\n",
    ")\n",
    "\n",
    "from Quick.cleaning.utils import (\n",
    "    get_file_path\n",
    ")\n",
    "\n",
    "from Quick.runners.deep import (\n",
    "    run_deep_nn_experiment\n",
    ")\n",
    "\n",
    "from Quick.runners.residual import (\n",
    "    run_residual_deep_nn_experiment\n",
    ")\n",
    "\n",
    "from Quick.runners.sk import (\n",
    "    run_sk_experiment\n",
    ")\n",
    "\n",
    "from Quick.runners.torch import (\n",
    "    run_torch_nn_experiment\n",
    ")\n",
    "\n",
    "from rff.layers import (\n",
    "    GaussianEncoding,\n",
    ")\n",
    "\n",
    "\n",
    "from Quick.constants import *\n",
    "\n",
    "pretty = pprint.PrettyPrinter(indent=4).pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are currently using the device: cpu\n"
     ]
    }
   ],
   "source": [
    "use_gpu: bool = False\n",
    "\n",
    "if(use_gpu):\n",
    "\n",
    "    if(torch.backends.mps.is_available()): # For Mac M1/M2 chips\n",
    "        device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    else: # For NVIDIA cuda chips\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'We are currently using the device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are currently using the CICDDoS-2019 dataset from the Canadian Institute of Cybersecurity, found [here](https://www.unb.ca/cic/datasets/ddos-2019.html). \n",
    "\n",
    "Download the csv files in zip form from the website and extract the zip in the ..data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datasets we will be working with are:\n",
      "[   '../data/01-12/DrDoS_NTP.csv',\n",
      "    '../data/01-12/DrDoS_DNS.csv',\n",
      "    '../data/01-12/DrDoS_LDAP.csv',\n",
      "    '../data/01-12/DrDoS_MSSQL.csv',\n",
      "    '../data/01-12/DrDoS_NetBIOS.csv',\n",
      "    '../data/01-12/DrDoS_SNMP.csv',\n",
      "    '../data/01-12/DrDoS_SSDP.csv',\n",
      "    '../data/01-12/DrDoS_UDP.csv',\n",
      "    '../data/01-12/UDPLag.csv',\n",
      "    '../data/01-12/Syn.csv',\n",
      "    '../data/01-12/TFTP.csv']\n",
      "and\n",
      "[   '../data/03-11/Portmap.csv',\n",
      "    '../data/03-11/NetBIOS.csv',\n",
      "    '../data/03-11/LDAP.csv',\n",
      "    '../data/03-11/MSSQL.csv',\n",
      "    '../data/03-11/UDP.csv',\n",
      "    '../data/03-11/UDPLAG.csv',\n",
      "    '../data/03-11/SYN.csv']\n"
     ]
    }
   ],
   "source": [
    "ddos_data_path_1: str = '../data/01-12/'\n",
    "ddos_data_path_2: str = '../data/03-11/'\n",
    "\n",
    "# Raw datasets, still contain infs/nans/empty and are all primarily 1 class\n",
    "ddos_data_1: list = [ # https://www.unb.ca/cic/datasets/ddos-2019.html\n",
    "    'DrDoS_NTP.csv',\n",
    "    'DrDoS_DNS.csv',\n",
    "    'DrDoS_LDAP.csv',\n",
    "    'DrDoS_MSSQL.csv',\n",
    "    'DrDoS_NetBIOS.csv',\n",
    "    'DrDoS_SNMP.csv',\n",
    "    'DrDoS_SSDP.csv',\n",
    "    'DrDoS_UDP.csv',\n",
    "    'UDPLag.csv',\n",
    "    'Syn.csv',\n",
    "    'TFTP.csv',\n",
    "]\n",
    "\n",
    "# Raw datasets, still contain infs/nans/empty and are all primarily 1 class\n",
    "ddos_data_2: list = [ # https://www.unb.ca/cic/datasets/ddos-2019.html\n",
    "    'Portmap.csv',\n",
    "    'NetBIOS.csv',\n",
    "    'LDAP.csv',\n",
    "    'MSSQL.csv',\n",
    "    'UDP.csv',\n",
    "    'UDPLAG.csv',\n",
    "    'SYN.csv'\n",
    "]\n",
    "\n",
    "\n",
    "# Now we package the datasets and load when necessary\n",
    "ddos_path_1: callable = get_file_path(ddos_data_path_1)\n",
    "ddos_path_2: callable = get_file_path(ddos_data_path_2)\n",
    "\n",
    "ddos_datasets_1: list = list(map(ddos_path_1, ddos_data_1))\n",
    "ddos_datasets_2: list = list(map(ddos_path_2, ddos_data_2))\n",
    "\n",
    "print(f'The datasets we will be working with are:')\n",
    "pretty(ddos_datasets_1)\n",
    "print('and')\n",
    "pretty(ddos_datasets_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "\n",
    "We define a class to handle the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    map_index_key = '__map_index__'\n",
    "    category_map = {}\n",
    "    old_maps = []\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline: List[Callable[[pd.DataFrame], pd.DataFrame]],\n",
    "        device: torch.device,\n",
    "    ) -> None:\n",
    "\n",
    "        self.pipeline = pipeline\n",
    "        self.device = device\n",
    "\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, leave_out: List[str] = ['Timestamp']):\n",
    "\n",
    "        X = X.copy()\n",
    "        for col in leave_out:\n",
    "            if col in X.columns:\n",
    "                X = X.drop(columns=[col])\n",
    "\n",
    "        for step in self.pipeline:\n",
    "            step.fit(X)\n",
    "\n",
    "        self.fitted = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(\n",
    "        self, \n",
    "        X: pd.DataFrame,\n",
    "        leave_out: List[str] = ['Timestamp']\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        if not self.fitted:\n",
    "            raise Exception('E1: You must fit the preprocessor before transforming')\n",
    "    \n",
    "        X = X.copy()\n",
    "        \n",
    "        if leave_out != []:\n",
    "            left_out = X[leave_out]\n",
    "            X = X.drop(columns=leave_out)\n",
    "\n",
    "        columns = X.columns\n",
    "\n",
    "        for col in leave_out:\n",
    "            if col in columns:\n",
    "                raise Exception('E2: Undesired column %s was found in the transformed dataset' % col)\n",
    "\n",
    "        for step in self.pipeline:\n",
    "            X = step.transform(X)\n",
    "\n",
    "        X = pd.DataFrame(X, columns=columns)\n",
    "\n",
    "        if leave_out != []:\n",
    "            for col in leave_out:\n",
    "                X[col] = left_out[col]\n",
    "            # X[leave_out] = left_out\n",
    "\n",
    "        for col in leave_out:\n",
    "            if col not in X.columns:\n",
    "                raise Exception('E3: Column %s was not found in the transformed dataset' % col)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def index_categories(\n",
    "        self, \n",
    "        X: pd.DataFrame, \n",
    "        categorical_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        '''\n",
    "            We need to index the categorical columns so that they are in the range [0, n_categories) and save the mapping\n",
    "        '''\n",
    "        X = X.copy()\n",
    "\n",
    "        if self.map_index_key in categorical_cols:\n",
    "            raise Exception('Cannot use the reserved key %s as a column name' % self.map_index_key)\n",
    "\n",
    "        if self.category_map != {}:\n",
    "            self.old_maps.append(self.category_map)\n",
    "        \n",
    "        old_mapping_index = len(self.old_maps)\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            X[col], self.category_map[col] = pd.factorize(X[col])\n",
    "            self.category_map[self.map_index_key] = old_mapping_index    \n",
    "\n",
    "        return X\n",
    "\n",
    "    def create_dataset(\n",
    "        self, \n",
    "        dataset: pd.DataFrame,\n",
    "        categorical_cols: List[str],\n",
    "        relative_cols: List[str],\n",
    "        other_cols: Dict[str, List[str]],\n",
    "        context_sequence_length: int = 1,\n",
    "        target_sequence_length: int = 1,\n",
    "        target_sequence_offset: int = 0,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        '''\n",
    "            We create a dataset for each column group where each shares the same index\n",
    "            in the first dimension\n",
    "\n",
    "            This is because the groups will have different lengths in the last dimension\n",
    "            \n",
    "\n",
    "            We also need to create a context and target sequence for each column group\n",
    "                where the context sequence is the input and the target sequence is the output\n",
    "            \n",
    "                for each sample s in the dataset:\n",
    "                    context_sequence = df[s:s+context_sequence_length]\n",
    "                    target_sequence = df[s+target_sequence_offset:s+target_sequence_offset+target_sequence_length]\n",
    "\n",
    "\n",
    "        '''\n",
    "        \n",
    "        if not self.fitted:\n",
    "            self.fit(\n",
    "                dataset, \n",
    "                leave_out=relative_cols + categorical_cols\n",
    "            )\n",
    "\n",
    "        dataset = self.transform(\n",
    "            dataset,\n",
    "            leave_out=relative_cols + categorical_cols\n",
    "        )\n",
    "\n",
    "        bounds = max(\n",
    "            context_sequence_length, \n",
    "            target_sequence_length + target_sequence_offset\n",
    "        )\n",
    "\n",
    "        dataset_groups = {}\n",
    "\n",
    "        for i, (name, group) in enumerate(other_cols.items()):\n",
    "            dataset_groups[name] = dataset[group]\n",
    "\n",
    "        categorical_data = dataset[categorical_cols]\n",
    "        categorical_data = self.index_categories(categorical_data, categorical_cols)\n",
    "\n",
    "        relative_data = dataset[relative_cols]\n",
    "\n",
    "        output = {}\n",
    "\n",
    "        src = '_source'\n",
    "        tgt = '_target'\n",
    "\n",
    "        output['categorical_data' + src] = []\n",
    "        output['categorical_data' + tgt] = []\n",
    "        output['relative_data' + src] = []\n",
    "        output['relative_data' + tgt] = []\n",
    "    \n",
    "        for name in other_cols.keys():\n",
    "            output[name + src] = []\n",
    "            output[name + tgt] = []\n",
    "\n",
    "        for i in range(len(dataset) - bounds):\n",
    "            \n",
    "            output['categorical_data' + src].append(\n",
    "                categorical_data[i:i+context_sequence_length].values\n",
    "            )\n",
    "\n",
    "            output['categorical_data' + tgt].append(\n",
    "                categorical_data[i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "            )\n",
    "\n",
    "            relative_source = relative_data[i:i+context_sequence_length].values\n",
    "\n",
    "            relative_target = relative_data[i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "\n",
    "            relative_source = relative_source - relative_source[0]\n",
    "            relative_target = relative_target - relative_source[0]\n",
    "\n",
    "            output['relative_data' + src].append(\n",
    "                relative_source\n",
    "            )\n",
    "\n",
    "            output['relative_data' + tgt].append(\n",
    "                relative_target\n",
    "            )\n",
    "            \n",
    "\n",
    "            for name in dataset_groups.keys():\n",
    "                output[name + src].append(\n",
    "                    dataset_groups[name][i:i+context_sequence_length].values\n",
    "                )\n",
    "\n",
    "                output[name + tgt].append(\n",
    "                    dataset_groups[name][i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "                )\n",
    "\n",
    "        for name in output.keys():\n",
    "            if name == 'categorical_data' + src or name == 'categorical_data' + tgt:\n",
    "                output[name] = torch.tensor(output[name], dtype=torch.long, device=self.device)\n",
    "            else:\n",
    "                output[name] = torch.tensor(output[name], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = examine_dataset(0, ddos_datasets_1.reverse(), ddos_data_1.reverse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the preprocessor, we must load and clean our dataset. \n",
    "\n",
    "Since this is a prototype, we only load 1 of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting 556264 rows with Infinity in column Flow Bytes/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ddos: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m \u001b[43mremove_infs_and_nans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m ddos\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [column\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m ddos\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m      4\u001b[0m timestamps \u001b[38;5;241m=\u001b[39m ddos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Programming/TrafficFlowForecasting/prototype/Quick/cleaning/loading.py:138\u001b[0m, in \u001b[0;36mremove_infs_and_nans\u001b[0;34m(data_summary)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    Function will return the dataset with all inf and nan values removed.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    137\u001b[0m df: pd\u001b[39m.\u001b[39mDataFrame \u001b[39m=\u001b[39m data_summary[\u001b[39m'\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m--> 138\u001b[0m df \u001b[39m=\u001b[39m clean_data(df, [])\n\u001b[1;32m    140\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/Programming/TrafficFlowForecasting/prototype/Quick/cleaning/utils.py:121\u001b[0m, in \u001b[0;36mclean_data\u001b[0;34m(df, prune)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns:\n\u001b[1;32m    120\u001b[0m     \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m invalid_values:\n\u001b[0;32m--> 121\u001b[0m         indexNames \u001b[39m=\u001b[39m df[df[col] \u001b[39m==\u001b[39;49m value]\u001b[39m.\u001b[39mindex\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexNames\u001b[39m.\u001b[39mempty:\n\u001b[1;32m    123\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdeleting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(indexNames)\u001b[39m}\u001b[39;00m\u001b[39m rows with Infinity in column \u001b[39m\u001b[39m{\u001b[39;00mcol\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast_gpu/lib/python3.9/site-packages/pandas/core/ops/common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     79\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast_gpu/lib/python3.9/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast_gpu/lib/python3.9/site-packages/pandas/core/series.py:6097\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6094\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   6096\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 6097\u001b[0m     res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[1;32m   6099\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast_gpu/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:286\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[39mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    285\u001b[0m \u001b[39melif\u001b[39;00m is_object_dtype(lvalues\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(rvalues, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     res_values \u001b[39m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[1;32m    288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast_gpu/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:75\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     73\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39mvec_compare(x\u001b[39m.\u001b[39mravel(), y\u001b[39m.\u001b[39mravel(), op)\n\u001b[1;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39;49mscalar_compare(x\u001b[39m.\u001b[39;49mravel(), y, op)\n\u001b[1;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddos: pd.DataFrame = remove_infs_and_nans(dataset_1)\n",
    "ddos.columns = [column.strip() for column in ddos.columns]\n",
    "\n",
    "timestamps = ddos['Timestamp']\n",
    "std_time = timestamps.apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f').timestamp())\n",
    "ddos['Timestamp'] = std_time\n",
    "\n",
    "ddos = ddos.sort_values(by=['Timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell contains all of the groups of columns. Each group will be used in a slightly different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns are accounted for\n"
     ]
    }
   ],
   "source": [
    "time_cols= [\n",
    "    'Timestamp',\n",
    "]\n",
    "\n",
    "speed_cols = [\n",
    "    'Flow Bytes/s',\n",
    "\n",
    "    'Bwd Packets/s',\n",
    "    'Fwd Packets/s',\n",
    "    'Flow Packets/s',\n",
    "    \n",
    "    'Down/Up Ratio',\n",
    "]\n",
    "\n",
    "length_cols = [\n",
    "    'ACK Flag Count',\n",
    "    'CWE Flag Count',\n",
    "    'ECE Flag Count',\n",
    "    'FIN Flag Count',\n",
    "    'PSH Flag Count',\n",
    "    'RST Flag Count',\n",
    "    'SYN Flag Count',\n",
    "    'URG Flag Count',\n",
    "    \n",
    "    'Flow Duration',\n",
    "\n",
    "    'Bwd PSH Flags',\n",
    "    'Fwd PSH Flags',\n",
    "    'Bwd URG Flags',\n",
    "    'Fwd URG Flags',\n",
    "\n",
    "    'Bwd Header Length',\n",
    "    \n",
    "    'Total Length of Fwd Packets',\n",
    "    'Total Length of Bwd Packets',\n",
    "    'Fwd Header Length',\n",
    "    \n",
    "    'Total Backward Packets',\n",
    "    'Total Fwd Packets',\n",
    "    'Subflow Bwd Packets',\n",
    "    'Subflow Fwd Packets',\n",
    "    \n",
    "    'Subflow Fwd Bytes',\n",
    "    'Subflow Bwd Bytes',\n",
    "\n",
    "    'Bwd IAT Total',\n",
    "    'Fwd IAT Total',\n",
    "\n",
    "    'act_data_pkt_fwd',\n",
    "    'Init_Win_bytes_backward',\n",
    "    'Init_Win_bytes_forward',\n",
    "]\n",
    "\n",
    "categorical_cols = {\n",
    "    # column: number_of_possible_classes\n",
    "    'Protocol': 256,\n",
    "    'Inbound': 2,\n",
    "}\n",
    "\n",
    "min_cols = [\n",
    "    'Active Min',\n",
    "    'Idle Min',\n",
    "\n",
    "    'Bwd IAT Min',\n",
    "    'Fwd IAT Min',\n",
    "    'Flow IAT Min',\n",
    "    \n",
    "    'Bwd Packet Length Min',\n",
    "    'Fwd Packet Length Min',\n",
    "    'Min Packet Length',\n",
    "\n",
    "    'min_seg_size_forward',\n",
    "]\n",
    "\n",
    "max_cols = [\n",
    "    'Active Max',\n",
    "    'Idle Max',\n",
    "\n",
    "    'Bwd IAT Max',\n",
    "    'Fwd IAT Max',\n",
    "    'Flow IAT Max',\n",
    "    \n",
    "    'Max Packet Length',\n",
    "    'Bwd Packet Length Max',\n",
    "    'Fwd Packet Length Max',\n",
    "]\n",
    "\n",
    "mean_cols = [\n",
    "    'Active Mean',\n",
    "    'Idle Mean',\n",
    "\n",
    "    'Bwd Avg Bulk Rate',\n",
    "    'Fwd Avg Bulk Rate',\n",
    "    \n",
    "    'Bwd Avg Bytes/Bulk',\n",
    "    'Fwd Avg Bytes/Bulk',\n",
    "    \n",
    "    'Bwd IAT Mean',\n",
    "    'Fwd IAT Mean',\n",
    "    'Flow IAT Mean',\n",
    "    \n",
    "    'Packet Length Mean',\n",
    "    'Bwd Packet Length Mean',\n",
    "    'Fwd Packet Length Mean',\n",
    "    \n",
    "    'Average Packet Size',\n",
    "    'Bwd Avg Packets/Bulk',\n",
    "    'Fwd Avg Packets/Bulk',\n",
    "    \n",
    "    'Avg Bwd Segment Size',\n",
    "    'Avg Fwd Segment Size',\n",
    "]\n",
    "\n",
    "std_cols = [\n",
    "    'Active Std',\n",
    "    'Idle Std',\n",
    "    \n",
    "    'Bwd IAT Std',\n",
    "    'Fwd IAT Std',\n",
    "    'Flow IAT Std',\n",
    "\n",
    "    'Packet Length Std',\n",
    "    'Packet Length Variance',\n",
    "    'Bwd Packet Length Std',\n",
    "    'Fwd Packet Length Std',\n",
    "]\n",
    "\n",
    "port_cols = [\n",
    "    'Destination Port',\n",
    "    'Source Port',\n",
    "]\n",
    "\n",
    "unused_cols = [\n",
    "    'Unnamed: 0',\n",
    "    'Flow ID',\n",
    "    'Source IP',\n",
    "    'Destination IP',\n",
    "    'SimillarHTTP',\n",
    "    'Label',\n",
    "    'Fwd Header Length.1'\n",
    "] + port_cols\n",
    "\n",
    "# concatenate all column groups together to get a list of all columns\n",
    "all_cols = speed_cols + port_cols + length_cols + time_cols + list(categorical_cols.keys()) + min_cols + max_cols + mean_cols + std_cols + unused_cols\n",
    "\n",
    "# all_cols = time_cols + categorical_cols + min_cols + max_cols + mean_cols + std_cols + length_cols + unused_cols\n",
    "\n",
    " \n",
    "if set(ddos.columns) - set(all_cols) != set():\n",
    "    print('There are unaccounted for columns')\n",
    "else:\n",
    "    print('All columns are accounted for')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_groups = {\n",
    "    'speed_cols': speed_cols,\n",
    "    'length_cols': length_cols,\n",
    "    'min_cols': min_cols,\n",
    "    'max_cols': max_cols,\n",
    "    'mean_cols': mean_cols,\n",
    "    'std_cols': std_cols,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddos_clean = ddos.copy().drop(columns=unused_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, now that we have a clean dataset and we know how we are grouping the columns, we can use the preprocessor on the data and create a dataset.\n",
    "\n",
    "The datasets will consist of context (or source) and target sequences. The offset determines the starting index of the target sequence relative to the context sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    map_index_key = '__map_index__'\n",
    "    category_map = {}\n",
    "    old_maps = []\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline: List[Callable[[pd.DataFrame], pd.DataFrame]],\n",
    "        device: torch.device,\n",
    "    ) -> None:\n",
    "\n",
    "        self.pipeline = pipeline\n",
    "        self.device = device\n",
    "\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, leave_out: List[str] = ['Timestamp']):\n",
    "\n",
    "        X = X.copy()\n",
    "        for col in leave_out:\n",
    "            if col in X.columns:\n",
    "                X = X.drop(columns=[col])\n",
    "\n",
    "        for step in self.pipeline:\n",
    "            step.fit(X)\n",
    "\n",
    "        self.fitted = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(\n",
    "        self, \n",
    "        df: pd.DataFrame,\n",
    "        leave_out: List[str] = ['Timestamp']\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        if not self.fitted:\n",
    "            raise Exception('E1: You must fit the preprocessor before transforming')\n",
    "    \n",
    "        X = df.copy()\n",
    "        df = df.copy()\n",
    "\n",
    "        if leave_out != []:\n",
    "            # print(leave_out)\n",
    "            left_out = X[leave_out].copy()\n",
    "            X = X.drop(columns=leave_out)\n",
    "\n",
    "        columns: list = X.columns\n",
    "\n",
    "        for col in leave_out:\n",
    "            if col in columns:\n",
    "                raise Exception('E2: Undesired column %s was found in the transformed dataset' % col)\n",
    "\n",
    "        for step in self.pipeline:\n",
    "            X: np.ndarray = step.transform(X)\n",
    "\n",
    "        X = pd.DataFrame(X, columns=columns)\n",
    "        # df[columns].values = X\n",
    "\n",
    "        # df = pd.DataFrame(np.concatenate(X, left_out.values), columns = columns.extend(leave_out))\n",
    "\n",
    "        # if leave_out != []:\n",
    "        #     for col in leave_out:\n",
    "        #         df[col] = left_out[col]\n",
    "        #         # X[col] = left_out[col]\n",
    "        #     # X[leave_out] = left_out\n",
    "\n",
    "        for col in columns:\n",
    "            df[col] = X[col]\n",
    "\n",
    "        for col in leave_out:\n",
    "            if col not in df.columns:\n",
    "                raise Exception('E3: Column %s was not found in the transformed dataset' % col)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def index_categories(\n",
    "        self, \n",
    "        X: pd.DataFrame, \n",
    "        categorical_cols: List[str]\n",
    "    ) -> pd.DataFrame:\n",
    "        '''\n",
    "            We need to index the categorical columns so that they are in the range [0, n_categories) and save the mapping\n",
    "        '''\n",
    "        X = X.copy()\n",
    "\n",
    "        if self.map_index_key in categorical_cols:\n",
    "            raise Exception('Cannot use the reserved key %s as a column name' % self.map_index_key)\n",
    "\n",
    "        if self.category_map != {}:\n",
    "            self.old_maps.append(self.category_map)\n",
    "        \n",
    "        old_mapping_index = len(self.old_maps)\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            X[col], self.category_map[col] = pd.factorize(X[col])\n",
    "            self.category_map[self.map_index_key] = old_mapping_index    \n",
    "\n",
    "        return X\n",
    "\n",
    "    def create_dataset(\n",
    "        self, \n",
    "        dataset: pd.DataFrame,\n",
    "        categorical_cols: List[str],\n",
    "        relative_cols: List[str],\n",
    "        other_cols: Dict[str, List[str]],\n",
    "        context_sequence_length: int = 1,\n",
    "        target_sequence_length: int = 1,\n",
    "        target_sequence_offset: int = 0,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        '''\n",
    "            We create a dataset for each column group where each shares the same index\n",
    "            in the first dimension\n",
    "\n",
    "            This is because the groups will have different lengths in the last dimension\n",
    "            \n",
    "\n",
    "            We also need to create a context and target sequence for each column group\n",
    "                where the context sequence is the input and the target sequence is the output\n",
    "            \n",
    "                for each sample s in the dataset:\n",
    "                    context_sequence = df[s:s+context_sequence_length]\n",
    "                    target_sequence = df[s+target_sequence_offset:s+target_sequence_offset+target_sequence_length]\n",
    "\n",
    "\n",
    "        '''\n",
    "        \n",
    "        output = {}\n",
    "\n",
    "        src = '_source'\n",
    "        tgt = '_target'\n",
    "\n",
    "        if not self.fitted:\n",
    "            self.fit(\n",
    "                dataset, \n",
    "                leave_out=relative_cols + categorical_cols\n",
    "            )\n",
    "\n",
    "        categorical_data = dataset[categorical_cols]\n",
    "        relative_data = dataset[relative_cols]\n",
    "        # print(categorical_data[100:])\n",
    "\n",
    "        dataset = self.transform(\n",
    "            dataset,\n",
    "            leave_out=relative_cols + categorical_cols\n",
    "        )\n",
    "\n",
    "        # categorical_data = dataset[categorical_cols]\n",
    "        # print(dataset[100:])\n",
    "        # print(categorical_data[100:])\n",
    "        \n",
    "        bounds = max(\n",
    "            context_sequence_length, \n",
    "            target_sequence_length + target_sequence_offset\n",
    "        )\n",
    "\n",
    "        dataset_groups = {}\n",
    "\n",
    "        for i, (name, group) in enumerate(other_cols.items()):\n",
    "            dataset_groups[name] = dataset[group]\n",
    "\n",
    "        # categorical_data = dataset[categorical_cols]\n",
    "        # print(categorical_data[100:])\n",
    "        # categorical_data = self.index_categories(categorical_data, categorical_cols)\n",
    "        # print(categorical_data[100:])\n",
    "\n",
    "\n",
    "        output['categorical_data' + src] = []\n",
    "        output['categorical_data' + tgt] = []\n",
    "        output['relative_data' + src] = []\n",
    "        output['relative_data' + tgt] = []\n",
    "    \n",
    "        for name in other_cols.keys():\n",
    "            output[name + src] = []\n",
    "            output[name + tgt] = []\n",
    "\n",
    "        for i in range(len(dataset) - bounds):\n",
    "            \n",
    "            output['categorical_data' + src].append(\n",
    "                categorical_data[i:i+context_sequence_length].values\n",
    "            )\n",
    "\n",
    "            output['categorical_data' + tgt].append(\n",
    "                categorical_data[i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "            )\n",
    "\n",
    "            relative_source = relative_data[i:i+context_sequence_length].values\n",
    "\n",
    "            relative_target = relative_data[i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "\n",
    "            relative_source = relative_source - relative_source[0]\n",
    "            relative_target = relative_target - relative_source[0]\n",
    "\n",
    "            output['relative_data' + src].append(\n",
    "                relative_source\n",
    "            )\n",
    "\n",
    "            output['relative_data' + tgt].append(\n",
    "                relative_target\n",
    "            )\n",
    "            \n",
    "\n",
    "            for name in dataset_groups.keys():\n",
    "                output[name + src].append(\n",
    "                    dataset_groups[name][i:i+context_sequence_length].values\n",
    "                )\n",
    "\n",
    "                output[name + tgt].append(\n",
    "                    dataset_groups[name][i+target_sequence_offset:i+target_sequence_offset+target_sequence_length].values\n",
    "                )\n",
    "\n",
    "        for name in output.keys():\n",
    "            if name == 'categorical_data' + src or name == 'categorical_data' + tgt:\n",
    "                # print(\"asdf\", output[name][-1])\n",
    "                # print(output[name])\n",
    "                output[name] = torch.tensor(output[name], dtype=torch.long, device=self.device)\n",
    "                # print(output[name][-1])\n",
    "            else:\n",
    "                output[name] = torch.tensor(output[name], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_data_source torch.Size([989, 10, 2])\n",
      "categorical_data_target torch.Size([989, 3, 2])\n",
      "relative_data_source torch.Size([989, 10, 1])\n",
      "relative_data_target torch.Size([989, 3, 1])\n",
      "speed_cols_source torch.Size([989, 10, 5])\n",
      "speed_cols_target torch.Size([989, 3, 5])\n",
      "length_cols_source torch.Size([989, 10, 28])\n",
      "length_cols_target torch.Size([989, 3, 28])\n",
      "min_cols_source torch.Size([989, 10, 9])\n",
      "min_cols_target torch.Size([989, 3, 9])\n",
      "max_cols_source torch.Size([989, 10, 8])\n",
      "max_cols_target torch.Size([989, 3, 8])\n",
      "mean_cols_source torch.Size([989, 10, 17])\n",
      "mean_cols_target torch.Size([989, 3, 17])\n",
      "std_cols_source torch.Size([989, 10, 9])\n",
      "std_cols_target torch.Size([989, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "process = Preprocessor([StandardScaler()], device)\n",
    "\n",
    "context_length = 10\n",
    "target_length = 3\n",
    "target_offset = 8\n",
    "\n",
    "data_dict = process.create_dataset(\n",
    "    ddos_clean[:1000], # we only select 10000 samples for now\n",
    "    list(categorical_cols.keys()),\n",
    "    time_cols,\n",
    "    column_groups,\n",
    "    context_sequence_length=context_length,\n",
    "    target_sequence_length=target_length,\n",
    "    target_sequence_offset=target_offset\n",
    ")\n",
    "\n",
    "for name, tensor in data_dict.items():\n",
    "    print(name, tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the dataset for consumption by our model by wrapping it in a handy torch container and splitting it into train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Protocol': Index([6.0, 17.0, 0.0], dtype='float64'),\n",
       " '__map_index__': 1,\n",
       " 'Inbound': Index([1.0, 0.0], dtype='float64')}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we use the TensorDataset function to wrap all of the different parts of dataset. \n",
    "# The first dimension must be the same size across all inputs\n",
    "tensor_dataset = data.TensorDataset(\n",
    "    data_dict['relative_data_source'],\n",
    "    data_dict['categorical_data_source'],\n",
    "    data_dict['speed_cols_source'],\n",
    "    data_dict['length_cols_source'],\n",
    "    data_dict['min_cols_source'],\n",
    "    data_dict['max_cols_source'],\n",
    "    data_dict['mean_cols_source'],\n",
    "    data_dict['std_cols_source'],\n",
    "    data_dict['relative_data_target'],\n",
    "    data_dict['categorical_data_target'],\n",
    "    data_dict['speed_cols_target'],\n",
    "    data_dict['length_cols_target'],\n",
    "    data_dict['min_cols_target'],\n",
    "    data_dict['max_cols_target'],\n",
    "    data_dict['mean_cols_target'],\n",
    "    data_dict['std_cols_target'],\n",
    ")\n",
    "\n",
    "\n",
    "# We can now randomly split off a train and test dataset from the TensorDataset\n",
    "train_size = int(len(tensor_dataset) * 0.8)\n",
    "test_size = len(tensor_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(tensor_dataset, [train_size, test_size])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a batch size, we create our dataloaders. These will randomize the minibatches for use by the model during each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = data.DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "Now that we have a dataset and we understand its structure, we can begin to build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedLoss(nn.Module):\n",
    "    \"Measures how well we have generated the sequence item\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        categorical_cols,\n",
    "        device\n",
    "    ):\n",
    "        super(GeneratedLoss, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "        self.categorical_cols = categorical_cols\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        preds, \n",
    "        cat_targs, \n",
    "        cont_targs\n",
    "    ):\n",
    "\n",
    "        cats, conts = preds\n",
    "        \n",
    "\n",
    "        tot_ce, pos = cats.new([0]), 0\n",
    "        for i, (k,v) in enumerate(self.cat_dict.items()):\n",
    "            tot_ce += self.ce(cats[:, pos:pos+v], cat_targs[:,i])\n",
    "            pos += v\n",
    "        \n",
    "        norm_cats = cats.new([len(self.cat_dict)])\n",
    "        norm_conts = conts.new([conts.size(1)])\n",
    "        cat_loss = tot_ce / norm_cats\n",
    "        cont_loss = self.mse(conts, cont_targs) / norm_conts\n",
    "        total = cat_loss + cont_loss\n",
    "\n",
    "        return total / cats.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        p: float = .05,\n",
    "    ):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, embeddings: List[torch.Tensor]) -> torch.Tensor:\n",
    "        print(f\"training: {self.training}\")\n",
    "\n",
    "        if not self.training:\n",
    "            return sum(embeddings)\n",
    "\n",
    "        else:\n",
    "            mask = torch.rand(\n",
    "                embeddings[0].shape[0],\n",
    "                embeddings[0].shape[1],\n",
    "                len(embeddings), \n",
    "                1, \n",
    "                device=embeddings[0].device\n",
    "            ) < self.p\n",
    "\n",
    "            mask = mask.float()\n",
    "\n",
    "            return sum([mask[:, :, i, :] * emb for i, emb in enumerate(embeddings)])\n",
    "\n",
    "\n",
    "class RandomFourierTimeEncoding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        input_size: int = 1,\n",
    "        encoding_half_size: int = 50,\n",
    "        sigma: float = 4,\n",
    "    ) -> None:\n",
    "        super(RandomFourierTimeEncoding, self).__init__()\n",
    "\n",
    "        self.fourier = GaussianEncoding(\n",
    "            sigma=sigma,\n",
    "            input_size=input_size,\n",
    "            encoded_size=encoding_half_size,\n",
    "        ).to(device)\n",
    "        # )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "            X: (batch_size, sequence_length, input_size)\n",
    "            output: (batch_size, sequence_length, 2*encoding_half_size + input_size)\n",
    "        '''\n",
    "\n",
    "        output = self.fourier(X)\n",
    "        output = torch.cat([X, output], dim=-1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrafficFlowTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        categorical_cols: Dict[str, int],\n",
    "        column_groups: Dict[str, List[str]],\n",
    "        device: torch.device,\n",
    "        dropout_rate: float = .1,\n",
    "        embedding_size: int = 128,\n",
    "        encoding_size: int = 50,\n",
    "        encoder_heads: int = 8,\n",
    "        encoder_layers: int = 2,\n",
    "        encoder_forward_expansion: int = 2,\n",
    "        decoder_heads: int = 8,\n",
    "        decoder_layers: int = 2,\n",
    "        decoder_forward_expansion: int = 2\n",
    "    ) -> None:\n",
    "    \n",
    "        super(TrafficFlowTransformer, self).__init__()\n",
    "    \n",
    "        self.categorical_cols = categorical_cols\n",
    "\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            RandomFourierTimeEncoding(\n",
    "                device=device,\n",
    "                input_size=1,\n",
    "                encoding_half_size=encoding_size,\n",
    "            ),\n",
    "            nn.Linear(2*encoding_size + 1, embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "        ).to(device)\n",
    "\n",
    "        self.categorical_embeddings = nn.ModuleDict({\n",
    "            col: nn.Embedding(num_embeddings=n_categories, embedding_dim=embedding_size)\n",
    "            for col, n_categories in categorical_cols.items()\n",
    "        }).to(device)\n",
    "\n",
    "        self.continuous_embeddings = nn.ModuleDict({\n",
    "            group: nn.Sequential(\n",
    "                nn.Linear(len(cols), embedding_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(embedding_size, embedding_size),\n",
    "            ) for group, cols in column_groups.items()\n",
    "        }).to(device)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_size,\n",
    "            nhead=encoder_heads,\n",
    "            num_encoder_layers=encoder_layers,\n",
    "            num_decoder_layers=decoder_layers,\n",
    "            dim_feedforward=encoder_forward_expansion*embedding_size,\n",
    "            dropout=dropout_rate,\n",
    "        ).to(device)\n",
    "\n",
    "        # self.encoder = self.transformer.encoder\n",
    "\n",
    "        self.emb_dropout = EmbeddingDropout()\n",
    "\n",
    "    def target_mask(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "            Here we make an additive mask for the target\n",
    "            X: (batch_size, sequence_length)\n",
    "            output: (sequence_length, sequence_length)\n",
    "        '''\n",
    "        sequence_length = X.shape[1]\n",
    "\n",
    "        mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
    "\n",
    "        return mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X_time,\n",
    "        X_cat,\n",
    "        X_cont_group: dict,\n",
    "        y_time,\n",
    "        y_cat,\n",
    "        y_cont_group: dict\n",
    "    ):\n",
    "        print(f\"X_time: {X_time.shape}\")\n",
    "        print(f\"X_cat: {X_cat.shape}\")\n",
    "        # print(f\"X_cont_group: {X_cont_group.shape}\")\n",
    "        print(f\"X_cont_group: {len(X_cont_group)}\")\n",
    "        print(f\"y_time: {y_time.shape}\")\n",
    "\n",
    "        # we embed the time\n",
    "        X_time_emb = self.time_embedding(X_time)\n",
    "        Y_time_emb = self.time_embedding(y_time)\n",
    "\n",
    "        print(f\"X_time_emb: {X_time_emb.shape}\")\n",
    "        print(f\"y_time_emb: {Y_time_emb.shape}\")\n",
    "\n",
    "        # we embed the categorical columns\n",
    "        # X_cat_embs = [\n",
    "        #     self.categorical_embeddings[col](X_cat[:, :, i])\n",
    "        #     for i, col in enumerate(self.categorical_cols.keys())\n",
    "        # ]\n",
    "\n",
    "        print(self.categorical_embeddings)\n",
    "\n",
    "        X_cat_embs = []\n",
    "        for i, col in enumerate(self.categorical_cols.keys()):\n",
    "            print(i, col, X_cat.size(), X_cat[:, :, i])\n",
    "            X_cat_embs.append(self.categorical_embeddings[col](X_cat[:, :, i]))\n",
    "\n",
    "        print(*[f\"X_cat_embs[{i}]: {emb.shape}\" for i, emb in enumerate(X_cat_embs)], sep='\\n')\n",
    "\n",
    "        # we embed the continuous columns\n",
    "        X_cont_embs = [\n",
    "            self.continuous_embeddings[group](X_cont_group[group])\n",
    "            for group in self.continuous_embeddings.keys()\n",
    "        ]\n",
    "\n",
    "        y_cont_embs = [\n",
    "            self.continuous_embeddings[group](y_cont_group[group])\n",
    "            for group in self.continuous_embeddings.keys()\n",
    "        ]\n",
    "\n",
    "        print(*[f\"X_cont_embs[{i}]: {emb.shape}\" for i, emb in enumerate(X_cont_embs)], sep='\\n')\n",
    "        print(*[f\"y_cont_embs[{i}]: {emb.shape}\" for i, emb in enumerate(y_cont_embs)], sep='\\n')\n",
    "\n",
    "        # print(f\"X_cat_embs: {[emb.shape for emb in X_cat_embs]}\")\n",
    "        # print(f\"X_cont_embs: {[emb.shape for emb in X_cont_embs]}\")\n",
    "\n",
    "        print(f'training: {self.training}')\n",
    "\n",
    "        X_emb = self.emb_dropout(X_cat_embs + X_cont_embs + [X_time_emb])\n",
    "\n",
    "        # X_emb = X_time_emb + sum(X_cat_embs) + sum(X_cont_embs)\n",
    "\n",
    "        print(f\"X_emb: {X_emb.shape}\")\n",
    "\n",
    "        # X_enc = self.encoder(X_emb)\n",
    "\n",
    "        print(f\"X_enc: {X_enc.shape}\")\n",
    "\n",
    "        # X_fourier = torch.fft.fft(X_enc.to(torch.device('cpu')), dim=-1)\n",
    "\n",
    "        # print(f\"X_fourier: {X_fourier.shape}\")\n",
    "\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_time: torch.Size([2, 10, 1])\n",
      "X_cat: torch.Size([2, 10, 2])\n",
      "X_cont_group: 6\n",
      "y_time: torch.Size([2, 3, 1])\n",
      "X_time_emb: torch.Size([2, 10, 128])\n",
      "y_time_emb: torch.Size([2, 3, 128])\n",
      "ModuleDict(\n",
      "  (Protocol): Embedding(256, 128)\n",
      "  (Inbound): Embedding(2, 128)\n",
      ")\n",
      "0 Protocol torch.Size([2, 10, 2]) tensor([[6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
      "        [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]])\n",
      "1 Inbound torch.Size([2, 10, 2]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "X_cat_embs[0]: torch.Size([2, 10, 128])\n",
      "X_cat_embs[1]: torch.Size([2, 10, 128])\n",
      "X_cont_embs[0]: torch.Size([2, 10, 128])\n",
      "X_cont_embs[1]: torch.Size([2, 10, 128])\n",
      "X_cont_embs[2]: torch.Size([2, 10, 128])\n",
      "X_cont_embs[3]: torch.Size([2, 10, 128])\n",
      "X_cont_embs[4]: torch.Size([2, 10, 128])\n",
      "X_cont_embs[5]: torch.Size([2, 10, 128])\n",
      "y_cont_embs[0]: torch.Size([2, 3, 128])\n",
      "y_cont_embs[1]: torch.Size([2, 3, 128])\n",
      "y_cont_embs[2]: torch.Size([2, 3, 128])\n",
      "y_cont_embs[3]: torch.Size([2, 3, 128])\n",
      "y_cont_embs[4]: torch.Size([2, 3, 128])\n",
      "y_cont_embs[5]: torch.Size([2, 3, 128])\n",
      "training: True\n",
      "training: True\n",
      "X_emb: torch.Size([2, 10, 128])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_enc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 64\u001b[0m\n\u001b[1;32m     59\u001b[0m y_cont_group_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     60\u001b[0m     group: y_cont_group[i] \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(column_groups\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     61\u001b[0m }\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# print(X_cat.size())\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m y_cat_pred, y_cont_group \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_cat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_cont_group_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_cat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_cont_group_dict\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m batch_0 \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fast_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[128], line 138\u001b[0m, in \u001b[0;36mTrafficFlowTransformer.forward\u001b[0;34m(self, X_time, X_cat, X_cont_group, y_time, y_cat, y_cont_group)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_emb: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_emb\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# X_enc = self.encoder(X_emb)\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_enc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_enc\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# X_fourier = torch.fft.fft(X_enc.to(torch.device('cpu')), dim=-1)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# print(f\"X_fourier: {X_fourier.shape}\")\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_enc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 10\n",
    "\n",
    "column_groups = {\n",
    "    'speed_cols': speed_cols,\n",
    "    'length_cols': length_cols,\n",
    "    'min_cols': min_cols,\n",
    "    'max_cols': max_cols,\n",
    "    'mean_cols': mean_cols,\n",
    "    'std_cols': std_cols,\n",
    "}\n",
    "\n",
    "categorical_cols = {\n",
    "    'Protocol': 256,\n",
    "    'Inbound': 2,\n",
    "}\n",
    "\n",
    "epoch_length = len(train_dataloader.dataset)\n",
    "\n",
    "train_losses = []\n",
    "test_losses  = []\n",
    "train_counter = []\n",
    "test_counter = [i * epoch_length for i in range(n_epochs + 1)]\n",
    "\n",
    "\n",
    "loss_fn = GeneratedLoss(\n",
    "    categorical_cols,\n",
    "    device\n",
    ")\n",
    "\n",
    "batch_0 = None\n",
    "\n",
    "model = TrafficFlowTransformer(\n",
    "    categorical_cols,\n",
    "    column_groups,\n",
    "    embedding_size=128,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# print(model)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    # model.eval()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        X_time = batch[0]\n",
    "        y_time = batch[8]\n",
    "\n",
    "        X_cat = batch[1]\n",
    "        y_cat = batch[9]\n",
    "\n",
    "        X_cont_group = batch[2:8]\n",
    "        X_cont_group_dict = {\n",
    "            group: X_cont_group[i] for i, group in enumerate(column_groups.keys())\n",
    "        }\n",
    "\n",
    "\n",
    "        y_cont_group = batch[10:]\n",
    "        y_cont_group_dict = {\n",
    "            group: y_cont_group[i] for i, group in enumerate(column_groups.keys())\n",
    "        }\n",
    "        # print(X_cat.size())\n",
    "\n",
    "        y_cat_pred, y_cont_group = model(\n",
    "            X_time,\n",
    "            X_cat,\n",
    "            X_cont_group_dict,\n",
    "            y_time,\n",
    "            y_cat,\n",
    "            y_cont_group_dict\n",
    "        )\n",
    "\n",
    "        batch_0 = batch\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('fast_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deaf3a77c670ae4bcde0102b8754366ef6094e7b1a1d30917591bfafe6a4ef0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
